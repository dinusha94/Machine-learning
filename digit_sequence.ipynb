{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_sequence.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxWc2FQ3ySIj0x9AVpI7ur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinusha94/Machine-learning/blob/master/digit_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_lCr8AblNfA",
        "colab_type": "code",
        "outputId": "d3bf9d03-2ca7-4b30-e646-e3340bb0fb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yq9Z8prpUVt",
        "colab_type": "code",
        "outputId": "17d8f44e-f74e-40c5-ceed-efad757d4ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# We're unzipping the cuDNN files from your Drive folder directly to the VM CUDA folders\n",
        "!tar -xzvf gdrive/My\\ Drive/darknet/cuDNN/cudnn-10.0-linux-x64-v7.5.0.56.tgz -C /usr/local/\n",
        "!chmod a+r /usr/local/cuda/include/cudnn.h\n",
        "\n",
        "# Now we check the version we already installed. Can comment this line on future runs\n",
        "!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda/include/cudnn.h\n",
            "cuda/NVIDIA_SLA_cuDNN_Support.txt\n",
            "cuda/lib64/libcudnn.so\n",
            "cuda/lib64/libcudnn.so.7\n",
            "cuda/lib64/libcudnn.so.7.5.0\n",
            "cuda/lib64/libcudnn_static.a\n",
            "#define CUDNN_MAJOR 7\n",
            "#define CUDNN_MINOR 5\n",
            "#define CUDNN_PATCHLEVEL 0\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "#include \"driver_types.h\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdbNrfApYj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.15 \n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FblhthK2pf2j",
        "colab_type": "code",
        "outputId": "cffb528d-c6ab-44e4-8b82-0aa328f4e4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkoBv2tjo9bl",
        "colab_type": "code",
        "outputId": "29445d6a-c058-4b87-a6b0-2211bf2955fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!gsutil cp -r /content/gdrive/My\\ Drive/train.npz /content/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/gdrive/My Drive/train.npz...\n",
            "/ [1 files][  2.2 MiB/  2.2 MiB]                                                \n",
            "Operation completed over 1 objects/2.2 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RYxN7zopFCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np      \n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import misc\n",
        "import PIL\n",
        "\n",
        "l = np.load('train.npz')\n",
        "\n",
        "# See what's in here\n",
        "#print (l.files)\n",
        "\n",
        "# Parse data\n",
        "train_img = l['data_img']\n",
        "train_label = l['data_labels']\n",
        "\n",
        "\n",
        "train_label = np.array(train_label)\n",
        "\n",
        "\n",
        "# reconstruct images\n",
        "image_data_list=[]\n",
        "frame = 0\n",
        "for im in train_img:\n",
        "    input_img = im.reshape(64, 28)\n",
        "    #cv2.imwrite(\"training_data/\"+str(frame)+\".jpg\", input_img)\n",
        "    image_data_list.append(input_img)\n",
        "    frame += 1\n",
        "\n",
        "\n",
        "img_data = np.array(image_data_list)\n",
        "#img_data = img_data.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "def build_synth_data(data,labels,dataset_size):\n",
        "    \n",
        "    #Define synthetic image dimensions\n",
        "    synth_img_height = 28\n",
        "    synth_img_width = 196\n",
        "    \n",
        "    #Define synthetic data\n",
        "    synth_data = np.ndarray(shape=(dataset_size,synth_img_height,synth_img_width),\n",
        "                           dtype=np.float32)\n",
        "    \n",
        "    #Define synthetic labels\n",
        "    synth_labels = [] \n",
        "    \n",
        "    #For a loop till the size of the synthetic dataset\n",
        "    for i in range(0,dataset_size):\n",
        "        \n",
        "        #Pick a random number of digits to be in the dataset\n",
        "        num_digits =  random.randint(1,7)\n",
        "        \n",
        "        #Randomly sampling indices to extract digits + labels afterwards\n",
        "        s_indices = [random.randint(0,len(data)-1) for p in range(0,num_digits)]\n",
        "        \n",
        "        #stitch images together\n",
        "        new_image = np.hstack([img_data[index] for index in s_indices])\n",
        "        \n",
        "        \n",
        "        #stitch the labels together\n",
        "        new_label =  [train_label[index] for index in s_indices]\n",
        "        \n",
        "        \n",
        "        #Loop till number of digits - 5, to concatenate blanks images, and blank labels together\n",
        "        for j in range(0, 7-num_digits):\n",
        "            new_image = np.hstack([new_image,np.zeros(shape=(64,28))])\n",
        "            new_label.append(10) #Might need to remove this step\n",
        "        \n",
        "        #Resize image\n",
        "        new_image = cv2.resize(new_image, (196,28), interpolation = cv2.INTER_AREA)\n",
        "        img2 = cv2.normalize(new_image,None,90,120,cv2.NORM_MINMAX)\n",
        "        blurImg = cv2.blur(img2,(2,2))  \n",
        "        #Assign the image to synth_data\n",
        "        synth_data[i,:,:] = blurImg\n",
        "        \n",
        "        #Assign the label to synth_data\n",
        "        synth_labels.append(tuple(new_label))\n",
        "        \n",
        "    \n",
        "    #Return the synthetic dataset\n",
        "    return synth_data,synth_labels\n",
        "\n",
        "\n",
        "\n",
        "#Building the training dataset\n",
        "X_synth_train,y_synth_train = build_synth_data(img_data,train_label,60000)\n",
        "#Building the test dataset\n",
        "X_synth_test,y_synth_test = build_synth_data(img_data,train_label,20000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hef0CpzMrkRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "bbba3679-16a1-4909-d1a2-02e93e10fcde"
      },
      "source": [
        "\n",
        "k = 13\n",
        "print(y_synth_train[k])\n",
        "plt.figure()\n",
        "plt.imshow(X_synth_train[k])\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([25.]), array([2.]), array([18.]), array([29.]), array([0.]), array([22.]), array([33.]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbc1d25ad68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABSCAYAAABNCo+2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRIHFP8fJ9wkvzwTYzMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUkUr381t8DX",
        "colab_type": "code",
        "outputId": "f4a450f8-5267-458d-f45b-db4fd81dcf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import h5py\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKRl3pkBsjfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "possible_classes = 37\n",
        "\n",
        "def convert_labels(labels):\n",
        "    \n",
        "    #As per Keras conventions, the multiple labels need to be of the form [array_digit1,...5]\n",
        "    #Each digit array will be of shape (60000,11)\n",
        "    \n",
        "    #Code below could be better, but cba for now. \n",
        "    \n",
        "    #Declare output ndarrays\n",
        "    dig0_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig1_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig2_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig3_arr = np.ndarray(shape=(len(labels),possible_classes))  \n",
        "    dig4_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig5_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    dig6_arr = np.ndarray(shape=(len(labels),possible_classes))\n",
        "    \n",
        "\n",
        "    for index,label in enumerate(labels):\n",
        "        \n",
        "        #Using np_utils from keras to OHE the labels in the image\n",
        "        dig0_arr[index,:] = np_utils.to_categorical(label[0],possible_classes)\n",
        "        dig1_arr[index,:] = np_utils.to_categorical(label[1],possible_classes)\n",
        "        dig2_arr[index,:] = np_utils.to_categorical(label[2],possible_classes)\n",
        "        dig3_arr[index,:] = np_utils.to_categorical(label[3],possible_classes)\n",
        "        dig4_arr[index,:] = np_utils.to_categorical(label[4],possible_classes)\n",
        "        dig5_arr[index,:] = np_utils.to_categorical(label[5],possible_classes)\n",
        "        dig6_arr[index,:] = np_utils.to_categorical(label[6],possible_classes)\n",
        "        \n",
        "    return [dig0_arr,dig1_arr,dig2_arr,dig3_arr,dig4_arr,dig5_arr,dig6_arr]\n",
        "\n",
        "    \n",
        "train_labels = convert_labels(y_synth_train)\n",
        "test_labels = convert_labels(y_synth_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmCgauoBrtgE",
        "colab_type": "code",
        "outputId": "b443c2be-2315-4866-da1a-cb83752982b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(train_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRXDWfnkuJT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep_data_keras(img_data):\n",
        "    \n",
        "    #Reshaping data for keras, with tensorflow as backend\n",
        "    img_data = img_data.reshape(len(img_data),196,28,1)\n",
        "    \n",
        "    #Converting everything to floats\n",
        "    img_data = img_data.astype('float32')\n",
        "    \n",
        "    #Normalizing values between 0 and 1\n",
        "    img_data /= 255\n",
        "    \n",
        "    return img_data\n",
        "\n",
        "train_images = prep_data_keras(X_synth_train)\n",
        "test_images = prep_data_keras(X_synth_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fdY2yhtuSiZ",
        "colab_type": "code",
        "outputId": "67edcaa6-9ef6-4a81-bf51-4ef9ec022c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(train_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 196, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMm0ejxDuWva",
        "colab_type": "code",
        "outputId": "b867f7a6-51b3-4e4e-a626-ac6b4eab8cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "#Building the model\n",
        "\n",
        "batch_size = 128\n",
        "nb_classes = 37\n",
        "nb_epoch = 25\n",
        "\n",
        "#image input dimensions\n",
        "img_rows = 196\n",
        "img_cols = 28\n",
        "img_channels = 1\n",
        "\n",
        "#number of convulation filters to use\n",
        "nb_filters = 32\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "#defining the input\n",
        "inputs = Input(shape=(img_rows,img_cols,img_channels))\n",
        "\n",
        "#Model taken from keras example. Worked well for a digit, dunno for multiple\n",
        "cov = Convolution2D(16,kernel_size=(3, 3),border_mode='same')(inputs)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size, strides=(2,2))(cov)\n",
        "#cov = Dropout(0.25)(cov)\n",
        "\n",
        "cov = Convolution2D(32,kernel_size=(3, 3))(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size, strides=(2,2))(cov)\n",
        "#cov = Dropout(0.25)(cov)\n",
        "\n",
        "cov = Convolution2D(64,kernel_size=(3, 3))(cov)\n",
        "cov = Activation('relu')(cov)\n",
        "cov = MaxPooling2D(pool_size=pool_size, strides=(2,2))(cov)\n",
        "\n",
        "\n",
        "cov_out = Flatten()(cov)\n",
        "\n",
        "\n",
        "#Dense Layers\n",
        "cov2 = Dense(1024, activation='relu')(cov_out)\n",
        "cov2 = Dropout(0.5)(cov2)\n",
        "\n",
        "\n",
        "\n",
        "#Prediction layers\n",
        "c0 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c1 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c2 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c3 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c4 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c5 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "c6 = Dense(nb_classes, activation='softmax')(cov2)\n",
        "\n",
        "#Defining the model\n",
        "model = Model(input=inputs,output=[c0,c1,c2,c3,c4,c5,c6])\n",
        "\n",
        "adm = adam(lr=0.0001)\n",
        "#Compiling the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer=adm,metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"seqence_wights.h5\", monitor='val_loss', verbose=1, save_best_only=True, \n",
        "                             save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "#Fitting the model\n",
        "model.fit(train_images,train_labels,batch_size=batch_size,nb_epoch=120,verbose=1,\n",
        "          validation_data=(test_images, test_labels), callbacks=[checkpoint])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, kernel_size=(3, 3), padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 20000 samples\n",
            "Epoch 1/120\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 25s 409us/step - loss: 17.2663 - dense_2_loss: 3.5452 - dense_3_loss: 3.3653 - dense_4_loss: 3.0274 - dense_5_loss: 2.6103 - dense_6_loss: 2.1301 - dense_7_loss: 1.6032 - dense_8_loss: 0.9847 - dense_2_acc: 0.0510 - dense_3_acc: 0.1706 - dense_4_acc: 0.3066 - dense_5_acc: 0.4397 - dense_6_acc: 0.5851 - dense_7_acc: 0.7199 - dense_8_acc: 0.8597 - val_loss: 15.0583 - val_dense_2_loss: 3.4913 - val_dense_3_loss: 3.1919 - val_dense_4_loss: 2.6687 - val_dense_5_loss: 2.1459 - val_dense_6_loss: 1.6609 - val_dense_7_loss: 1.1902 - val_dense_8_loss: 0.7094 - val_dense_2_acc: 0.0682 - val_dense_3_acc: 0.1845 - val_dense_4_acc: 0.3205 - val_dense_5_acc: 0.4545 - val_dense_6_acc: 0.5897 - val_dense_7_acc: 0.7261 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 15.05825, saving model to seqence_wights.h5\n",
            "Epoch 2/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 14.8488 - dense_2_loss: 3.5330 - dense_3_loss: 3.1534 - dense_4_loss: 2.6163 - dense_5_loss: 2.0901 - dense_6_loss: 1.6104 - dense_7_loss: 1.1474 - dense_8_loss: 0.6983 - dense_2_acc: 0.0550 - dense_3_acc: 0.1854 - dense_4_acc: 0.3220 - dense_5_acc: 0.4551 - dense_6_acc: 0.5902 - dense_7_acc: 0.7250 - dense_8_acc: 0.8620 - val_loss: 14.3725 - val_dense_2_loss: 3.4611 - val_dense_3_loss: 3.0144 - val_dense_4_loss: 2.5090 - val_dense_5_loss: 2.0435 - val_dense_6_loss: 1.5669 - val_dense_7_loss: 1.0994 - val_dense_8_loss: 0.6782 - val_dense_2_acc: 0.0734 - val_dense_3_acc: 0.1953 - val_dense_4_acc: 0.3370 - val_dense_5_acc: 0.4593 - val_dense_6_acc: 0.5923 - val_dense_7_acc: 0.7265 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00002: val_loss improved from 15.05825 to 14.37252, saving model to seqence_wights.h5\n",
            "Epoch 3/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 14.4739 - dense_2_loss: 3.4991 - dense_3_loss: 3.0539 - dense_4_loss: 2.5475 - dense_5_loss: 2.0502 - dense_6_loss: 1.5670 - dense_7_loss: 1.0910 - dense_8_loss: 0.6652 - dense_2_acc: 0.0628 - dense_3_acc: 0.1903 - dense_4_acc: 0.3272 - dense_5_acc: 0.4592 - dense_6_acc: 0.5934 - dense_7_acc: 0.7262 - dense_8_acc: 0.8620 - val_loss: 14.1442 - val_dense_2_loss: 3.4396 - val_dense_3_loss: 2.9684 - val_dense_4_loss: 2.4764 - val_dense_5_loss: 2.0171 - val_dense_6_loss: 1.5421 - val_dense_7_loss: 1.0571 - val_dense_8_loss: 0.6436 - val_dense_2_acc: 0.1013 - val_dense_3_acc: 0.2084 - val_dense_4_acc: 0.3399 - val_dense_5_acc: 0.4621 - val_dense_6_acc: 0.5934 - val_dense_7_acc: 0.7281 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00003: val_loss improved from 14.37252 to 14.14420, saving model to seqence_wights.h5\n",
            "Epoch 4/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 14.2529 - dense_2_loss: 3.4702 - dense_3_loss: 3.0104 - dense_4_loss: 2.5114 - dense_5_loss: 2.0233 - dense_6_loss: 1.5471 - dense_7_loss: 1.0574 - dense_8_loss: 0.6332 - dense_2_acc: 0.0691 - dense_3_acc: 0.1973 - dense_4_acc: 0.3327 - dense_5_acc: 0.4623 - dense_6_acc: 0.5943 - dense_7_acc: 0.7274 - dense_8_acc: 0.8620 - val_loss: 13.9521 - val_dense_2_loss: 3.4060 - val_dense_3_loss: 2.9318 - val_dense_4_loss: 2.4446 - val_dense_5_loss: 1.9939 - val_dense_6_loss: 1.5253 - val_dense_7_loss: 1.0328 - val_dense_8_loss: 0.6178 - val_dense_2_acc: 0.1187 - val_dense_3_acc: 0.2167 - val_dense_4_acc: 0.3463 - val_dense_5_acc: 0.4660 - val_dense_6_acc: 0.5938 - val_dense_7_acc: 0.7301 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00004: val_loss improved from 14.14420 to 13.95209, saving model to seqence_wights.h5\n",
            "Epoch 5/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 13.9977 - dense_2_loss: 3.4057 - dense_3_loss: 2.9539 - dense_4_loss: 2.4668 - dense_5_loss: 1.9967 - dense_6_loss: 1.5303 - dense_7_loss: 1.0386 - dense_8_loss: 0.6056 - dense_2_acc: 0.0907 - dense_3_acc: 0.2106 - dense_4_acc: 0.3428 - dense_5_acc: 0.4672 - dense_6_acc: 0.5972 - dense_7_acc: 0.7287 - dense_8_acc: 0.8620 - val_loss: 13.6640 - val_dense_2_loss: 3.3025 - val_dense_3_loss: 2.8579 - val_dense_4_loss: 2.3867 - val_dense_5_loss: 1.9669 - val_dense_6_loss: 1.5203 - val_dense_7_loss: 1.0292 - val_dense_8_loss: 0.6005 - val_dense_2_acc: 0.1311 - val_dense_3_acc: 0.2417 - val_dense_4_acc: 0.3675 - val_dense_5_acc: 0.4733 - val_dense_6_acc: 0.5998 - val_dense_7_acc: 0.7320 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00005: val_loss improved from 13.95209 to 13.66397, saving model to seqence_wights.h5\n",
            "Epoch 6/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 13.4548 - dense_2_loss: 3.2047 - dense_3_loss: 2.8166 - dense_4_loss: 2.3547 - dense_5_loss: 1.9551 - dense_6_loss: 1.5131 - dense_7_loss: 1.0315 - dense_8_loss: 0.5792 - dense_2_acc: 0.1464 - dense_3_acc: 0.2425 - dense_4_acc: 0.3701 - dense_5_acc: 0.4785 - dense_6_acc: 0.6019 - dense_7_acc: 0.7302 - dense_8_acc: 0.8620 - val_loss: 12.7510 - val_dense_2_loss: 2.9512 - val_dense_3_loss: 2.6325 - val_dense_4_loss: 2.2043 - val_dense_5_loss: 1.8999 - val_dense_6_loss: 1.4823 - val_dense_7_loss: 1.0105 - val_dense_8_loss: 0.5703 - val_dense_2_acc: 0.2421 - val_dense_3_acc: 0.2873 - val_dense_4_acc: 0.4142 - val_dense_5_acc: 0.4943 - val_dense_6_acc: 0.6095 - val_dense_7_acc: 0.7330 - val_dense_8_acc: 0.8614\n",
            "\n",
            "Epoch 00006: val_loss improved from 13.66397 to 12.75099, saving model to seqence_wights.h5\n",
            "Epoch 7/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 12.4529 - dense_2_loss: 2.7859 - dense_3_loss: 2.5503 - dense_4_loss: 2.1587 - dense_5_loss: 1.8866 - dense_6_loss: 1.4784 - dense_7_loss: 1.0232 - dense_8_loss: 0.5698 - dense_2_acc: 0.2523 - dense_3_acc: 0.3014 - dense_4_acc: 0.4145 - dense_5_acc: 0.4964 - dense_6_acc: 0.6096 - dense_7_acc: 0.7334 - dense_8_acc: 0.8622 - val_loss: 11.4711 - val_dense_2_loss: 2.4151 - val_dense_3_loss: 2.2984 - val_dense_4_loss: 1.9592 - val_dense_5_loss: 1.8010 - val_dense_6_loss: 1.4348 - val_dense_7_loss: 1.0010 - val_dense_8_loss: 0.5617 - val_dense_2_acc: 0.3952 - val_dense_3_acc: 0.3928 - val_dense_4_acc: 0.4830 - val_dense_5_acc: 0.5215 - val_dense_6_acc: 0.6200 - val_dense_7_acc: 0.7376 - val_dense_8_acc: 0.8616\n",
            "\n",
            "Epoch 00007: val_loss improved from 12.75099 to 11.47105, saving model to seqence_wights.h5\n",
            "Epoch 8/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 11.3094 - dense_2_loss: 2.3135 - dense_3_loss: 2.2529 - dense_4_loss: 1.9369 - dense_5_loss: 1.8010 - dense_6_loss: 1.4326 - dense_7_loss: 1.0133 - dense_8_loss: 0.5590 - dense_2_acc: 0.3708 - dense_3_acc: 0.3775 - dense_4_acc: 0.4695 - dense_5_acc: 0.5161 - dense_6_acc: 0.6235 - dense_7_acc: 0.7359 - dense_8_acc: 0.8639 - val_loss: 10.1880 - val_dense_2_loss: 1.9416 - val_dense_3_loss: 1.9642 - val_dense_4_loss: 1.7009 - val_dense_5_loss: 1.6854 - val_dense_6_loss: 1.3710 - val_dense_7_loss: 0.9824 - val_dense_8_loss: 0.5426 - val_dense_2_acc: 0.5279 - val_dense_3_acc: 0.4898 - val_dense_4_acc: 0.5624 - val_dense_5_acc: 0.5486 - val_dense_6_acc: 0.6373 - val_dense_7_acc: 0.7420 - val_dense_8_acc: 0.8649\n",
            "\n",
            "Epoch 00008: val_loss improved from 11.47105 to 10.18801, saving model to seqence_wights.h5\n",
            "Epoch 9/120\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 10.2262 - dense_2_loss: 1.9283 - dense_3_loss: 1.9539 - dense_4_loss: 1.7161 - dense_5_loss: 1.7042 - dense_6_loss: 1.3786 - dense_7_loss: 1.0039 - dense_8_loss: 0.5413 - dense_2_acc: 0.4734 - dense_3_acc: 0.4545 - dense_4_acc: 0.5251 - dense_5_acc: 0.5383 - dense_6_acc: 0.6360 - dense_7_acc: 0.7387 - dense_8_acc: 0.8659 - val_loss: 9.0232 - val_dense_2_loss: 1.5763 - val_dense_3_loss: 1.6433 - val_dense_4_loss: 1.4598 - val_dense_5_loss: 1.5563 - val_dense_6_loss: 1.2980 - val_dense_7_loss: 0.9660 - val_dense_8_loss: 0.5234 - val_dense_2_acc: 0.6171 - val_dense_3_acc: 0.5714 - val_dense_4_acc: 0.6260 - val_dense_5_acc: 0.5820 - val_dense_6_acc: 0.6518 - val_dense_7_acc: 0.7458 - val_dense_8_acc: 0.8686\n",
            "\n",
            "Epoch 00009: val_loss improved from 10.18801 to 9.02321, saving model to seqence_wights.h5\n",
            "Epoch 10/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 9.3424 - dense_2_loss: 1.6564 - dense_3_loss: 1.7054 - dense_4_loss: 1.5304 - dense_5_loss: 1.6055 - dense_6_loss: 1.3277 - dense_7_loss: 0.9909 - dense_8_loss: 0.5259 - dense_2_acc: 0.5445 - dense_3_acc: 0.5195 - dense_4_acc: 0.5731 - dense_5_acc: 0.5618 - dense_6_acc: 0.6481 - dense_7_acc: 0.7424 - dense_8_acc: 0.8679 - val_loss: 8.1391 - val_dense_2_loss: 1.3398 - val_dense_3_loss: 1.4010 - val_dense_4_loss: 1.2734 - val_dense_5_loss: 1.4397 - val_dense_6_loss: 1.2314 - val_dense_7_loss: 0.9474 - val_dense_8_loss: 0.5065 - val_dense_2_acc: 0.6645 - val_dense_3_acc: 0.6363 - val_dense_4_acc: 0.6725 - val_dense_5_acc: 0.6106 - val_dense_6_acc: 0.6698 - val_dense_7_acc: 0.7494 - val_dense_8_acc: 0.8710\n",
            "\n",
            "Epoch 00010: val_loss improved from 9.02321 to 8.13905, saving model to seqence_wights.h5\n",
            "Epoch 11/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 8.6236 - dense_2_loss: 1.4638 - dense_3_loss: 1.5031 - dense_4_loss: 1.3812 - dense_5_loss: 1.5108 - dense_6_loss: 1.2754 - dense_7_loss: 0.9785 - dense_8_loss: 0.5107 - dense_2_acc: 0.5967 - dense_3_acc: 0.5728 - dense_4_acc: 0.6133 - dense_5_acc: 0.5820 - dense_6_acc: 0.6571 - dense_7_acc: 0.7457 - dense_8_acc: 0.8707 - val_loss: 7.4229 - val_dense_2_loss: 1.1731 - val_dense_3_loss: 1.2063 - val_dense_4_loss: 1.1244 - val_dense_5_loss: 1.3198 - val_dense_6_loss: 1.1683 - val_dense_7_loss: 0.9359 - val_dense_8_loss: 0.4951 - val_dense_2_acc: 0.7031 - val_dense_3_acc: 0.6958 - val_dense_4_acc: 0.7088 - val_dense_5_acc: 0.6427 - val_dense_6_acc: 0.6851 - val_dense_7_acc: 0.7520 - val_dense_8_acc: 0.8728\n",
            "\n",
            "Epoch 00011: val_loss improved from 8.13905 to 7.42289, saving model to seqence_wights.h5\n",
            "Epoch 12/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 8.0214 - dense_2_loss: 1.3242 - dense_3_loss: 1.3382 - dense_4_loss: 1.2563 - dense_5_loss: 1.4163 - dense_6_loss: 1.2245 - dense_7_loss: 0.9646 - dense_8_loss: 0.4973 - dense_2_acc: 0.6322 - dense_3_acc: 0.6211 - dense_4_acc: 0.6468 - dense_5_acc: 0.6064 - dense_6_acc: 0.6695 - dense_7_acc: 0.7492 - dense_8_acc: 0.8724 - val_loss: 6.8466 - val_dense_2_loss: 1.0473 - val_dense_3_loss: 1.0655 - val_dense_4_loss: 1.0053 - val_dense_5_loss: 1.2200 - val_dense_6_loss: 1.1135 - val_dense_7_loss: 0.9149 - val_dense_8_loss: 0.4801 - val_dense_2_acc: 0.7366 - val_dense_3_acc: 0.7304 - val_dense_4_acc: 0.7437 - val_dense_5_acc: 0.6724 - val_dense_6_acc: 0.6994 - val_dense_7_acc: 0.7577 - val_dense_8_acc: 0.8763\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.42289 to 6.84663, saving model to seqence_wights.h5\n",
            "Epoch 13/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 7.5142 - dense_2_loss: 1.2094 - dense_3_loss: 1.2080 - dense_4_loss: 1.1541 - dense_5_loss: 1.3309 - dense_6_loss: 1.1799 - dense_7_loss: 0.9460 - dense_8_loss: 0.4859 - dense_2_acc: 0.6616 - dense_3_acc: 0.6559 - dense_4_acc: 0.6707 - dense_5_acc: 0.6260 - dense_6_acc: 0.6766 - dense_7_acc: 0.7518 - dense_8_acc: 0.8747 - val_loss: 6.3223 - val_dense_2_loss: 0.9423 - val_dense_3_loss: 0.9454 - val_dense_4_loss: 0.9065 - val_dense_5_loss: 1.1224 - val_dense_6_loss: 1.0552 - val_dense_7_loss: 0.8833 - val_dense_8_loss: 0.4671 - val_dense_2_acc: 0.7599 - val_dense_3_acc: 0.7647 - val_dense_4_acc: 0.7722 - val_dense_5_acc: 0.6991 - val_dense_6_acc: 0.7148 - val_dense_7_acc: 0.7645 - val_dense_8_acc: 0.8780\n",
            "\n",
            "Epoch 00013: val_loss improved from 6.84663 to 6.32226, saving model to seqence_wights.h5\n",
            "Epoch 14/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 7.0485 - dense_2_loss: 1.1109 - dense_3_loss: 1.1005 - dense_4_loss: 1.0613 - dense_5_loss: 1.2469 - dense_6_loss: 1.1281 - dense_7_loss: 0.9264 - dense_8_loss: 0.4745 - dense_2_acc: 0.6878 - dense_3_acc: 0.6870 - dense_4_acc: 0.6964 - dense_5_acc: 0.6479 - dense_6_acc: 0.6878 - dense_7_acc: 0.7563 - dense_8_acc: 0.8768 - val_loss: 5.8916 - val_dense_2_loss: 0.8486 - val_dense_3_loss: 0.8491 - val_dense_4_loss: 0.8235 - val_dense_5_loss: 1.0382 - val_dense_6_loss: 1.0095 - val_dense_7_loss: 0.8654 - val_dense_8_loss: 0.4573 - val_dense_2_acc: 0.7851 - val_dense_3_acc: 0.7895 - val_dense_4_acc: 0.7983 - val_dense_5_acc: 0.7257 - val_dense_6_acc: 0.7228 - val_dense_7_acc: 0.7687 - val_dense_8_acc: 0.8805\n",
            "\n",
            "Epoch 00014: val_loss improved from 6.32226 to 5.89163, saving model to seqence_wights.h5\n",
            "Epoch 15/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 6.6236 - dense_2_loss: 1.0205 - dense_3_loss: 1.0054 - dense_4_loss: 0.9808 - dense_5_loss: 1.1633 - dense_6_loss: 1.0847 - dense_7_loss: 0.9029 - dense_8_loss: 0.4660 - dense_2_acc: 0.7121 - dense_3_acc: 0.7126 - dense_4_acc: 0.7208 - dense_5_acc: 0.6691 - dense_6_acc: 0.6966 - dense_7_acc: 0.7590 - dense_8_acc: 0.8786 - val_loss: 5.4704 - val_dense_2_loss: 0.7699 - val_dense_3_loss: 0.7640 - val_dense_4_loss: 0.7459 - val_dense_5_loss: 0.9560 - val_dense_6_loss: 0.9531 - val_dense_7_loss: 0.8362 - val_dense_8_loss: 0.4451 - val_dense_2_acc: 0.8023 - val_dense_3_acc: 0.8119 - val_dense_4_acc: 0.8177 - val_dense_5_acc: 0.7516 - val_dense_6_acc: 0.7367 - val_dense_7_acc: 0.7728 - val_dense_8_acc: 0.8840\n",
            "\n",
            "Epoch 00015: val_loss improved from 5.89163 to 5.47042, saving model to seqence_wights.h5\n",
            "Epoch 16/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 6.2546 - dense_2_loss: 0.9467 - dense_3_loss: 0.9335 - dense_4_loss: 0.9074 - dense_5_loss: 1.0884 - dense_6_loss: 1.0429 - dense_7_loss: 0.8803 - dense_8_loss: 0.4554 - dense_2_acc: 0.7313 - dense_3_acc: 0.7331 - dense_4_acc: 0.7410 - dense_5_acc: 0.6868 - dense_6_acc: 0.7082 - dense_7_acc: 0.7638 - dense_8_acc: 0.8801 - val_loss: 5.0993 - val_dense_2_loss: 0.6989 - val_dense_3_loss: 0.6969 - val_dense_4_loss: 0.6749 - val_dense_5_loss: 0.8742 - val_dense_6_loss: 0.8983 - val_dense_7_loss: 0.8167 - val_dense_8_loss: 0.4394 - val_dense_2_acc: 0.8188 - val_dense_3_acc: 0.8243 - val_dense_4_acc: 0.8373 - val_dense_5_acc: 0.7773 - val_dense_6_acc: 0.7547 - val_dense_7_acc: 0.7813 - val_dense_8_acc: 0.8847\n",
            "\n",
            "Epoch 00016: val_loss improved from 5.47042 to 5.09932, saving model to seqence_wights.h5\n",
            "Epoch 17/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 5.9051 - dense_2_loss: 0.8738 - dense_3_loss: 0.8695 - dense_4_loss: 0.8483 - dense_5_loss: 1.0178 - dense_6_loss: 0.9949 - dense_7_loss: 0.8539 - dense_8_loss: 0.4468 - dense_2_acc: 0.7483 - dense_3_acc: 0.7510 - dense_4_acc: 0.7562 - dense_5_acc: 0.7076 - dense_6_acc: 0.7176 - dense_7_acc: 0.7691 - dense_8_acc: 0.8817 - val_loss: 4.7454 - val_dense_2_loss: 0.6278 - val_dense_3_loss: 0.6279 - val_dense_4_loss: 0.6161 - val_dense_5_loss: 0.8083 - val_dense_6_loss: 0.8536 - val_dense_7_loss: 0.7842 - val_dense_8_loss: 0.4276 - val_dense_2_acc: 0.8415 - val_dense_3_acc: 0.8448 - val_dense_4_acc: 0.8558 - val_dense_5_acc: 0.7935 - val_dense_6_acc: 0.7680 - val_dense_7_acc: 0.7903 - val_dense_8_acc: 0.8874\n",
            "\n",
            "Epoch 00017: val_loss improved from 5.09932 to 4.74538, saving model to seqence_wights.h5\n",
            "Epoch 18/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 5.5765 - dense_2_loss: 0.8111 - dense_3_loss: 0.8049 - dense_4_loss: 0.7902 - dense_5_loss: 0.9508 - dense_6_loss: 0.9520 - dense_7_loss: 0.8296 - dense_8_loss: 0.4378 - dense_2_acc: 0.7648 - dense_3_acc: 0.7679 - dense_4_acc: 0.7728 - dense_5_acc: 0.7242 - dense_6_acc: 0.7287 - dense_7_acc: 0.7746 - dense_8_acc: 0.8837 - val_loss: 4.4212 - val_dense_2_loss: 0.5671 - val_dense_3_loss: 0.5728 - val_dense_4_loss: 0.5673 - val_dense_5_loss: 0.7434 - val_dense_6_loss: 0.7986 - val_dense_7_loss: 0.7546 - val_dense_8_loss: 0.4174 - val_dense_2_acc: 0.8592 - val_dense_3_acc: 0.8592 - val_dense_4_acc: 0.8640 - val_dense_5_acc: 0.8119 - val_dense_6_acc: 0.7849 - val_dense_7_acc: 0.7969 - val_dense_8_acc: 0.8890\n",
            "\n",
            "Epoch 00018: val_loss improved from 4.74538 to 4.42118, saving model to seqence_wights.h5\n",
            "Epoch 19/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 5.2850 - dense_2_loss: 0.7581 - dense_3_loss: 0.7496 - dense_4_loss: 0.7391 - dense_5_loss: 0.8930 - dense_6_loss: 0.9080 - dense_7_loss: 0.8067 - dense_8_loss: 0.4306 - dense_2_acc: 0.7802 - dense_3_acc: 0.7833 - dense_4_acc: 0.7865 - dense_5_acc: 0.7420 - dense_6_acc: 0.7378 - dense_7_acc: 0.7781 - dense_8_acc: 0.8849 - val_loss: 4.1315 - val_dense_2_loss: 0.5163 - val_dense_3_loss: 0.5194 - val_dense_4_loss: 0.5187 - val_dense_5_loss: 0.6819 - val_dense_6_loss: 0.7471 - val_dense_7_loss: 0.7344 - val_dense_8_loss: 0.4137 - val_dense_2_acc: 0.8718 - val_dense_3_acc: 0.8718 - val_dense_4_acc: 0.8769 - val_dense_5_acc: 0.8295 - val_dense_6_acc: 0.7940 - val_dense_7_acc: 0.8027 - val_dense_8_acc: 0.8899\n",
            "\n",
            "Epoch 00019: val_loss improved from 4.42118 to 4.13145, saving model to seqence_wights.h5\n",
            "Epoch 20/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 5.0124 - dense_2_loss: 0.7083 - dense_3_loss: 0.7035 - dense_4_loss: 0.6975 - dense_5_loss: 0.8398 - dense_6_loss: 0.8627 - dense_7_loss: 0.7822 - dense_8_loss: 0.4184 - dense_2_acc: 0.7934 - dense_3_acc: 0.7930 - dense_4_acc: 0.7959 - dense_5_acc: 0.7552 - dense_6_acc: 0.7503 - dense_7_acc: 0.7847 - dense_8_acc: 0.8876 - val_loss: 3.8491 - val_dense_2_loss: 0.4668 - val_dense_3_loss: 0.4784 - val_dense_4_loss: 0.4768 - val_dense_5_loss: 0.6300 - val_dense_6_loss: 0.7034 - val_dense_7_loss: 0.6966 - val_dense_8_loss: 0.3972 - val_dense_2_acc: 0.8821 - val_dense_3_acc: 0.8806 - val_dense_4_acc: 0.8873 - val_dense_5_acc: 0.8394 - val_dense_6_acc: 0.8099 - val_dense_7_acc: 0.8115 - val_dense_8_acc: 0.8937\n",
            "\n",
            "Epoch 00020: val_loss improved from 4.13145 to 3.84910, saving model to seqence_wights.h5\n",
            "Epoch 21/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 4.7646 - dense_2_loss: 0.6620 - dense_3_loss: 0.6660 - dense_4_loss: 0.6597 - dense_5_loss: 0.7897 - dense_6_loss: 0.8215 - dense_7_loss: 0.7545 - dense_8_loss: 0.4112 - dense_2_acc: 0.8058 - dense_3_acc: 0.8059 - dense_4_acc: 0.8077 - dense_5_acc: 0.7680 - dense_6_acc: 0.7600 - dense_7_acc: 0.7902 - dense_8_acc: 0.8896 - val_loss: 3.6163 - val_dense_2_loss: 0.4304 - val_dense_3_loss: 0.4408 - val_dense_4_loss: 0.4418 - val_dense_5_loss: 0.5798 - val_dense_6_loss: 0.6571 - val_dense_7_loss: 0.6736 - val_dense_8_loss: 0.3927 - val_dense_2_acc: 0.8907 - val_dense_3_acc: 0.8871 - val_dense_4_acc: 0.8935 - val_dense_5_acc: 0.8548 - val_dense_6_acc: 0.8234 - val_dense_7_acc: 0.8131 - val_dense_8_acc: 0.8951\n",
            "\n",
            "Epoch 00021: val_loss improved from 3.84910 to 3.61627, saving model to seqence_wights.h5\n",
            "Epoch 22/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 4.5214 - dense_2_loss: 0.6170 - dense_3_loss: 0.6269 - dense_4_loss: 0.6227 - dense_5_loss: 0.7413 - dense_6_loss: 0.7811 - dense_7_loss: 0.7309 - dense_8_loss: 0.4016 - dense_2_acc: 0.8177 - dense_3_acc: 0.8157 - dense_4_acc: 0.8174 - dense_5_acc: 0.7814 - dense_6_acc: 0.7707 - dense_7_acc: 0.7946 - dense_8_acc: 0.8928 - val_loss: 3.4320 - val_dense_2_loss: 0.4032 - val_dense_3_loss: 0.4113 - val_dense_4_loss: 0.4191 - val_dense_5_loss: 0.5471 - val_dense_6_loss: 0.6245 - val_dense_7_loss: 0.6461 - val_dense_8_loss: 0.3807 - val_dense_2_acc: 0.8983 - val_dense_3_acc: 0.9005 - val_dense_4_acc: 0.9030 - val_dense_5_acc: 0.8698 - val_dense_6_acc: 0.8339 - val_dense_7_acc: 0.8239 - val_dense_8_acc: 0.8989\n",
            "\n",
            "Epoch 00022: val_loss improved from 3.61627 to 3.43203, saving model to seqence_wights.h5\n",
            "Epoch 23/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 4.3039 - dense_2_loss: 0.5836 - dense_3_loss: 0.5927 - dense_4_loss: 0.5938 - dense_5_loss: 0.6993 - dense_6_loss: 0.7439 - dense_7_loss: 0.6979 - dense_8_loss: 0.3926 - dense_2_acc: 0.8261 - dense_3_acc: 0.8258 - dense_4_acc: 0.8252 - dense_5_acc: 0.7938 - dense_6_acc: 0.7804 - dense_7_acc: 0.8021 - dense_8_acc: 0.8931 - val_loss: 3.1869 - val_dense_2_loss: 0.3634 - val_dense_3_loss: 0.3777 - val_dense_4_loss: 0.3847 - val_dense_5_loss: 0.5015 - val_dense_6_loss: 0.5756 - val_dense_7_loss: 0.6139 - val_dense_8_loss: 0.3703 - val_dense_2_acc: 0.9089 - val_dense_3_acc: 0.9065 - val_dense_4_acc: 0.9078 - val_dense_5_acc: 0.8784 - val_dense_6_acc: 0.8471 - val_dense_7_acc: 0.8320 - val_dense_8_acc: 0.9019\n",
            "\n",
            "Epoch 00023: val_loss improved from 3.43203 to 3.18694, saving model to seqence_wights.h5\n",
            "Epoch 24/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 4.1076 - dense_2_loss: 0.5520 - dense_3_loss: 0.5607 - dense_4_loss: 0.5623 - dense_5_loss: 0.6621 - dense_6_loss: 0.7048 - dense_7_loss: 0.6793 - dense_8_loss: 0.3864 - dense_2_acc: 0.8354 - dense_3_acc: 0.8336 - dense_4_acc: 0.8337 - dense_5_acc: 0.8033 - dense_6_acc: 0.7907 - dense_7_acc: 0.8088 - dense_8_acc: 0.8949 - val_loss: 2.9838 - val_dense_2_loss: 0.3328 - val_dense_3_loss: 0.3449 - val_dense_4_loss: 0.3549 - val_dense_5_loss: 0.4624 - val_dense_6_loss: 0.5368 - val_dense_7_loss: 0.5885 - val_dense_8_loss: 0.3635 - val_dense_2_acc: 0.9157 - val_dense_3_acc: 0.9130 - val_dense_4_acc: 0.9139 - val_dense_5_acc: 0.8864 - val_dense_6_acc: 0.8597 - val_dense_7_acc: 0.8385 - val_dense_8_acc: 0.9032\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.18694 to 2.98382, saving model to seqence_wights.h5\n",
            "Epoch 25/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 3.9123 - dense_2_loss: 0.5179 - dense_3_loss: 0.5296 - dense_4_loss: 0.5395 - dense_5_loss: 0.6246 - dense_6_loss: 0.6722 - dense_7_loss: 0.6511 - dense_8_loss: 0.3773 - dense_2_acc: 0.8450 - dense_3_acc: 0.8414 - dense_4_acc: 0.8402 - dense_5_acc: 0.8136 - dense_6_acc: 0.7987 - dense_7_acc: 0.8147 - dense_8_acc: 0.8977 - val_loss: 2.8174 - val_dense_2_loss: 0.3088 - val_dense_3_loss: 0.3240 - val_dense_4_loss: 0.3361 - val_dense_5_loss: 0.4330 - val_dense_6_loss: 0.5043 - val_dense_7_loss: 0.5586 - val_dense_8_loss: 0.3525 - val_dense_2_acc: 0.9212 - val_dense_3_acc: 0.9163 - val_dense_4_acc: 0.9177 - val_dense_5_acc: 0.8942 - val_dense_6_acc: 0.8699 - val_dense_7_acc: 0.8482 - val_dense_8_acc: 0.9061\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.98382 to 2.81742, saving model to seqence_wights.h5\n",
            "Epoch 26/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 3.7440 - dense_2_loss: 0.4898 - dense_3_loss: 0.5091 - dense_4_loss: 0.5081 - dense_5_loss: 0.5936 - dense_6_loss: 0.6421 - dense_7_loss: 0.6308 - dense_8_loss: 0.3706 - dense_2_acc: 0.8516 - dense_3_acc: 0.8481 - dense_4_acc: 0.8484 - dense_5_acc: 0.8207 - dense_6_acc: 0.8059 - dense_7_acc: 0.8192 - dense_8_acc: 0.8991 - val_loss: 2.6562 - val_dense_2_loss: 0.2888 - val_dense_3_loss: 0.2992 - val_dense_4_loss: 0.3137 - val_dense_5_loss: 0.4009 - val_dense_6_loss: 0.4672 - val_dense_7_loss: 0.5389 - val_dense_8_loss: 0.3473 - val_dense_2_acc: 0.9255 - val_dense_3_acc: 0.9239 - val_dense_4_acc: 0.9248 - val_dense_5_acc: 0.9039 - val_dense_6_acc: 0.8760 - val_dense_7_acc: 0.8525 - val_dense_8_acc: 0.9073\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.81742 to 2.65618, saving model to seqence_wights.h5\n",
            "Epoch 27/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 3.5851 - dense_2_loss: 0.4705 - dense_3_loss: 0.4863 - dense_4_loss: 0.4914 - dense_5_loss: 0.5631 - dense_6_loss: 0.6052 - dense_7_loss: 0.6062 - dense_8_loss: 0.3623 - dense_2_acc: 0.8566 - dense_3_acc: 0.8558 - dense_4_acc: 0.8527 - dense_5_acc: 0.8315 - dense_6_acc: 0.8182 - dense_7_acc: 0.8255 - dense_8_acc: 0.9002 - val_loss: 2.4913 - val_dense_2_loss: 0.2644 - val_dense_3_loss: 0.2789 - val_dense_4_loss: 0.2909 - val_dense_5_loss: 0.3742 - val_dense_6_loss: 0.4374 - val_dense_7_loss: 0.5085 - val_dense_8_loss: 0.3370 - val_dense_2_acc: 0.9333 - val_dense_3_acc: 0.9293 - val_dense_4_acc: 0.9297 - val_dense_5_acc: 0.9084 - val_dense_6_acc: 0.8882 - val_dense_7_acc: 0.8592 - val_dense_8_acc: 0.9089\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.65618 to 2.49132, saving model to seqence_wights.h5\n",
            "Epoch 28/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 3.4402 - dense_2_loss: 0.4458 - dense_3_loss: 0.4653 - dense_4_loss: 0.4700 - dense_5_loss: 0.5385 - dense_6_loss: 0.5829 - dense_7_loss: 0.5853 - dense_8_loss: 0.3524 - dense_2_acc: 0.8658 - dense_3_acc: 0.8591 - dense_4_acc: 0.8593 - dense_5_acc: 0.8394 - dense_6_acc: 0.8243 - dense_7_acc: 0.8308 - dense_8_acc: 0.9023 - val_loss: 2.3701 - val_dense_2_loss: 0.2483 - val_dense_3_loss: 0.2667 - val_dense_4_loss: 0.2757 - val_dense_5_loss: 0.3552 - val_dense_6_loss: 0.4123 - val_dense_7_loss: 0.4861 - val_dense_8_loss: 0.3260 - val_dense_2_acc: 0.9385 - val_dense_3_acc: 0.9316 - val_dense_4_acc: 0.9341 - val_dense_5_acc: 0.9130 - val_dense_6_acc: 0.8976 - val_dense_7_acc: 0.8703 - val_dense_8_acc: 0.9115\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.49132 to 2.37013, saving model to seqence_wights.h5\n",
            "Epoch 29/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 3.2943 - dense_2_loss: 0.4305 - dense_3_loss: 0.4420 - dense_4_loss: 0.4480 - dense_5_loss: 0.5162 - dense_6_loss: 0.5529 - dense_7_loss: 0.5605 - dense_8_loss: 0.3442 - dense_2_acc: 0.8689 - dense_3_acc: 0.8656 - dense_4_acc: 0.8657 - dense_5_acc: 0.8447 - dense_6_acc: 0.8326 - dense_7_acc: 0.8373 - dense_8_acc: 0.9041 - val_loss: 2.2309 - val_dense_2_loss: 0.2330 - val_dense_3_loss: 0.2444 - val_dense_4_loss: 0.2605 - val_dense_5_loss: 0.3286 - val_dense_6_loss: 0.3798 - val_dense_7_loss: 0.4641 - val_dense_8_loss: 0.3205 - val_dense_2_acc: 0.9396 - val_dense_3_acc: 0.9367 - val_dense_4_acc: 0.9354 - val_dense_5_acc: 0.9198 - val_dense_6_acc: 0.9043 - val_dense_7_acc: 0.8722 - val_dense_8_acc: 0.9133\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.37013 to 2.23094, saving model to seqence_wights.h5\n",
            "Epoch 30/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 3.1691 - dense_2_loss: 0.4077 - dense_3_loss: 0.4276 - dense_4_loss: 0.4343 - dense_5_loss: 0.4900 - dense_6_loss: 0.5287 - dense_7_loss: 0.5421 - dense_8_loss: 0.3386 - dense_2_acc: 0.8752 - dense_3_acc: 0.8705 - dense_4_acc: 0.8683 - dense_5_acc: 0.8506 - dense_6_acc: 0.8395 - dense_7_acc: 0.8417 - dense_8_acc: 0.9046 - val_loss: 2.1287 - val_dense_2_loss: 0.2189 - val_dense_3_loss: 0.2317 - val_dense_4_loss: 0.2494 - val_dense_5_loss: 0.3110 - val_dense_6_loss: 0.3650 - val_dense_7_loss: 0.4418 - val_dense_8_loss: 0.3109 - val_dense_2_acc: 0.9457 - val_dense_3_acc: 0.9425 - val_dense_4_acc: 0.9380 - val_dense_5_acc: 0.9261 - val_dense_6_acc: 0.9102 - val_dense_7_acc: 0.8816 - val_dense_8_acc: 0.9163\n",
            "\n",
            "Epoch 00030: val_loss improved from 2.23094 to 2.12869, saving model to seqence_wights.h5\n",
            "Epoch 31/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 3.0425 - dense_2_loss: 0.3946 - dense_3_loss: 0.4086 - dense_4_loss: 0.4168 - dense_5_loss: 0.4730 - dense_6_loss: 0.5023 - dense_7_loss: 0.5193 - dense_8_loss: 0.3280 - dense_2_acc: 0.8787 - dense_3_acc: 0.8741 - dense_4_acc: 0.8730 - dense_5_acc: 0.8568 - dense_6_acc: 0.8467 - dense_7_acc: 0.8470 - dense_8_acc: 0.9077 - val_loss: 2.0078 - val_dense_2_loss: 0.2058 - val_dense_3_loss: 0.2174 - val_dense_4_loss: 0.2355 - val_dense_5_loss: 0.2906 - val_dense_6_loss: 0.3396 - val_dense_7_loss: 0.4198 - val_dense_8_loss: 0.2991 - val_dense_2_acc: 0.9475 - val_dense_3_acc: 0.9445 - val_dense_4_acc: 0.9413 - val_dense_5_acc: 0.9311 - val_dense_6_acc: 0.9153 - val_dense_7_acc: 0.8880 - val_dense_8_acc: 0.9179\n",
            "\n",
            "Epoch 00031: val_loss improved from 2.12869 to 2.00776, saving model to seqence_wights.h5\n",
            "Epoch 32/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 2.9335 - dense_2_loss: 0.3762 - dense_3_loss: 0.3917 - dense_4_loss: 0.4041 - dense_5_loss: 0.4557 - dense_6_loss: 0.4850 - dense_7_loss: 0.5004 - dense_8_loss: 0.3204 - dense_2_acc: 0.8841 - dense_3_acc: 0.8800 - dense_4_acc: 0.8755 - dense_5_acc: 0.8616 - dense_6_acc: 0.8507 - dense_7_acc: 0.8518 - dense_8_acc: 0.9099 - val_loss: 1.8992 - val_dense_2_loss: 0.1904 - val_dense_3_loss: 0.2022 - val_dense_4_loss: 0.2197 - val_dense_5_loss: 0.2703 - val_dense_6_loss: 0.3166 - val_dense_7_loss: 0.4030 - val_dense_8_loss: 0.2971 - val_dense_2_acc: 0.9514 - val_dense_3_acc: 0.9487 - val_dense_4_acc: 0.9460 - val_dense_5_acc: 0.9356 - val_dense_6_acc: 0.9186 - val_dense_7_acc: 0.8910 - val_dense_8_acc: 0.9189\n",
            "\n",
            "Epoch 00032: val_loss improved from 2.00776 to 1.89916, saving model to seqence_wights.h5\n",
            "Epoch 33/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 2.8224 - dense_2_loss: 0.3637 - dense_3_loss: 0.3798 - dense_4_loss: 0.3901 - dense_5_loss: 0.4341 - dense_6_loss: 0.4613 - dense_7_loss: 0.4793 - dense_8_loss: 0.3140 - dense_2_acc: 0.8878 - dense_3_acc: 0.8829 - dense_4_acc: 0.8817 - dense_5_acc: 0.8679 - dense_6_acc: 0.8583 - dense_7_acc: 0.8576 - dense_8_acc: 0.9107 - val_loss: 1.8130 - val_dense_2_loss: 0.1817 - val_dense_3_loss: 0.1948 - val_dense_4_loss: 0.2091 - val_dense_5_loss: 0.2584 - val_dense_6_loss: 0.2995 - val_dense_7_loss: 0.3805 - val_dense_8_loss: 0.2890 - val_dense_2_acc: 0.9546 - val_dense_3_acc: 0.9509 - val_dense_4_acc: 0.9476 - val_dense_5_acc: 0.9375 - val_dense_6_acc: 0.9270 - val_dense_7_acc: 0.8998 - val_dense_8_acc: 0.9217\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.89916 to 1.81300, saving model to seqence_wights.h5\n",
            "Epoch 34/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 2.7284 - dense_2_loss: 0.3496 - dense_3_loss: 0.3658 - dense_4_loss: 0.3761 - dense_5_loss: 0.4204 - dense_6_loss: 0.4454 - dense_7_loss: 0.4653 - dense_8_loss: 0.3058 - dense_2_acc: 0.8922 - dense_3_acc: 0.8876 - dense_4_acc: 0.8844 - dense_5_acc: 0.8719 - dense_6_acc: 0.8624 - dense_7_acc: 0.8619 - dense_8_acc: 0.9127 - val_loss: 1.7072 - val_dense_2_loss: 0.1690 - val_dense_3_loss: 0.1788 - val_dense_4_loss: 0.1951 - val_dense_5_loss: 0.2456 - val_dense_6_loss: 0.2786 - val_dense_7_loss: 0.3628 - val_dense_8_loss: 0.2771 - val_dense_2_acc: 0.9568 - val_dense_3_acc: 0.9541 - val_dense_4_acc: 0.9506 - val_dense_5_acc: 0.9395 - val_dense_6_acc: 0.9312 - val_dense_7_acc: 0.9015 - val_dense_8_acc: 0.9246\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.81300 to 1.70715, saving model to seqence_wights.h5\n",
            "Epoch 35/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 2.6305 - dense_2_loss: 0.3365 - dense_3_loss: 0.3509 - dense_4_loss: 0.3647 - dense_5_loss: 0.4042 - dense_6_loss: 0.4256 - dense_7_loss: 0.4495 - dense_8_loss: 0.2991 - dense_2_acc: 0.8960 - dense_3_acc: 0.8911 - dense_4_acc: 0.8873 - dense_5_acc: 0.8756 - dense_6_acc: 0.8687 - dense_7_acc: 0.8653 - dense_8_acc: 0.9132 - val_loss: 1.6130 - val_dense_2_loss: 0.1589 - val_dense_3_loss: 0.1667 - val_dense_4_loss: 0.1838 - val_dense_5_loss: 0.2305 - val_dense_6_loss: 0.2617 - val_dense_7_loss: 0.3434 - val_dense_8_loss: 0.2680 - val_dense_2_acc: 0.9587 - val_dense_3_acc: 0.9578 - val_dense_4_acc: 0.9541 - val_dense_5_acc: 0.9437 - val_dense_6_acc: 0.9356 - val_dense_7_acc: 0.9092 - val_dense_8_acc: 0.9258\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.70715 to 1.61299, saving model to seqence_wights.h5\n",
            "Epoch 36/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 2.5533 - dense_2_loss: 0.3293 - dense_3_loss: 0.3424 - dense_4_loss: 0.3532 - dense_5_loss: 0.3935 - dense_6_loss: 0.4107 - dense_7_loss: 0.4329 - dense_8_loss: 0.2913 - dense_2_acc: 0.8974 - dense_3_acc: 0.8938 - dense_4_acc: 0.8915 - dense_5_acc: 0.8799 - dense_6_acc: 0.8727 - dense_7_acc: 0.8707 - dense_8_acc: 0.9170 - val_loss: 1.5650 - val_dense_2_loss: 0.1559 - val_dense_3_loss: 0.1645 - val_dense_4_loss: 0.1800 - val_dense_5_loss: 0.2211 - val_dense_6_loss: 0.2520 - val_dense_7_loss: 0.3301 - val_dense_8_loss: 0.2615 - val_dense_2_acc: 0.9594 - val_dense_3_acc: 0.9587 - val_dense_4_acc: 0.9546 - val_dense_5_acc: 0.9484 - val_dense_6_acc: 0.9385 - val_dense_7_acc: 0.9139 - val_dense_8_acc: 0.9281\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.61299 to 1.56501, saving model to seqence_wights.h5\n",
            "Epoch 37/120\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 2.4654 - dense_2_loss: 0.3187 - dense_3_loss: 0.3258 - dense_4_loss: 0.3403 - dense_5_loss: 0.3801 - dense_6_loss: 0.3955 - dense_7_loss: 0.4203 - dense_8_loss: 0.2848 - dense_2_acc: 0.9011 - dense_3_acc: 0.8993 - dense_4_acc: 0.8946 - dense_5_acc: 0.8824 - dense_6_acc: 0.8770 - dense_7_acc: 0.8726 - dense_8_acc: 0.9178 - val_loss: 1.4804 - val_dense_2_loss: 0.1463 - val_dense_3_loss: 0.1550 - val_dense_4_loss: 0.1705 - val_dense_5_loss: 0.2076 - val_dense_6_loss: 0.2366 - val_dense_7_loss: 0.3142 - val_dense_8_loss: 0.2502 - val_dense_2_acc: 0.9625 - val_dense_3_acc: 0.9615 - val_dense_4_acc: 0.9561 - val_dense_5_acc: 0.9502 - val_dense_6_acc: 0.9425 - val_dense_7_acc: 0.9164 - val_dense_8_acc: 0.9317\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.56501 to 1.48044, saving model to seqence_wights.h5\n",
            "Epoch 38/120\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 2.3850 - dense_2_loss: 0.3093 - dense_3_loss: 0.3198 - dense_4_loss: 0.3339 - dense_5_loss: 0.3643 - dense_6_loss: 0.3809 - dense_7_loss: 0.4006 - dense_8_loss: 0.2761 - dense_2_acc: 0.9043 - dense_3_acc: 0.9006 - dense_4_acc: 0.8960 - dense_5_acc: 0.8867 - dense_6_acc: 0.8813 - dense_7_acc: 0.8788 - dense_8_acc: 0.9194 - val_loss: 1.4214 - val_dense_2_loss: 0.1381 - val_dense_3_loss: 0.1482 - val_dense_4_loss: 0.1642 - val_dense_5_loss: 0.1995 - val_dense_6_loss: 0.2243 - val_dense_7_loss: 0.3013 - val_dense_8_loss: 0.2458 - val_dense_2_acc: 0.9647 - val_dense_3_acc: 0.9635 - val_dense_4_acc: 0.9579 - val_dense_5_acc: 0.9525 - val_dense_6_acc: 0.9465 - val_dense_7_acc: 0.9222 - val_dense_8_acc: 0.9318\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.48044 to 1.42136, saving model to seqence_wights.h5\n",
            "Epoch 39/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 2.3149 - dense_2_loss: 0.2961 - dense_3_loss: 0.3067 - dense_4_loss: 0.3231 - dense_5_loss: 0.3534 - dense_6_loss: 0.3729 - dense_7_loss: 0.3906 - dense_8_loss: 0.2721 - dense_2_acc: 0.9073 - dense_3_acc: 0.9037 - dense_4_acc: 0.8998 - dense_5_acc: 0.8906 - dense_6_acc: 0.8834 - dense_7_acc: 0.8820 - dense_8_acc: 0.9210 - val_loss: 1.3565 - val_dense_2_loss: 0.1319 - val_dense_3_loss: 0.1397 - val_dense_4_loss: 0.1588 - val_dense_5_loss: 0.1860 - val_dense_6_loss: 0.2155 - val_dense_7_loss: 0.2865 - val_dense_8_loss: 0.2382 - val_dense_2_acc: 0.9670 - val_dense_3_acc: 0.9652 - val_dense_4_acc: 0.9588 - val_dense_5_acc: 0.9568 - val_dense_6_acc: 0.9476 - val_dense_7_acc: 0.9237 - val_dense_8_acc: 0.9340\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.42136 to 1.35654, saving model to seqence_wights.h5\n",
            "Epoch 40/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 2.2402 - dense_2_loss: 0.2868 - dense_3_loss: 0.3001 - dense_4_loss: 0.3166 - dense_5_loss: 0.3431 - dense_6_loss: 0.3547 - dense_7_loss: 0.3739 - dense_8_loss: 0.2649 - dense_2_acc: 0.9095 - dense_3_acc: 0.9050 - dense_4_acc: 0.9005 - dense_5_acc: 0.8930 - dense_6_acc: 0.8891 - dense_7_acc: 0.8851 - dense_8_acc: 0.9224 - val_loss: 1.2983 - val_dense_2_loss: 0.1255 - val_dense_3_loss: 0.1332 - val_dense_4_loss: 0.1476 - val_dense_5_loss: 0.1792 - val_dense_6_loss: 0.2023 - val_dense_7_loss: 0.2773 - val_dense_8_loss: 0.2332 - val_dense_2_acc: 0.9671 - val_dense_3_acc: 0.9658 - val_dense_4_acc: 0.9631 - val_dense_5_acc: 0.9579 - val_dense_6_acc: 0.9522 - val_dense_7_acc: 0.9274 - val_dense_8_acc: 0.9352\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.35654 to 1.29831, saving model to seqence_wights.h5\n",
            "Epoch 41/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 2.1745 - dense_2_loss: 0.2811 - dense_3_loss: 0.2901 - dense_4_loss: 0.3040 - dense_5_loss: 0.3306 - dense_6_loss: 0.3449 - dense_7_loss: 0.3644 - dense_8_loss: 0.2593 - dense_2_acc: 0.9114 - dense_3_acc: 0.9085 - dense_4_acc: 0.9036 - dense_5_acc: 0.8962 - dense_6_acc: 0.8928 - dense_7_acc: 0.8886 - dense_8_acc: 0.9235 - val_loss: 1.2364 - val_dense_2_loss: 0.1179 - val_dense_3_loss: 0.1289 - val_dense_4_loss: 0.1399 - val_dense_5_loss: 0.1712 - val_dense_6_loss: 0.1935 - val_dense_7_loss: 0.2602 - val_dense_8_loss: 0.2249 - val_dense_2_acc: 0.9694 - val_dense_3_acc: 0.9677 - val_dense_4_acc: 0.9641 - val_dense_5_acc: 0.9592 - val_dense_6_acc: 0.9544 - val_dense_7_acc: 0.9331 - val_dense_8_acc: 0.9369\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.29831 to 1.23641, saving model to seqence_wights.h5\n",
            "Epoch 42/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 2.1180 - dense_2_loss: 0.2704 - dense_3_loss: 0.2826 - dense_4_loss: 0.2992 - dense_5_loss: 0.3229 - dense_6_loss: 0.3345 - dense_7_loss: 0.3543 - dense_8_loss: 0.2541 - dense_2_acc: 0.9161 - dense_3_acc: 0.9098 - dense_4_acc: 0.9063 - dense_5_acc: 0.8995 - dense_6_acc: 0.8941 - dense_7_acc: 0.8922 - dense_8_acc: 0.9260 - val_loss: 1.1832 - val_dense_2_loss: 0.1134 - val_dense_3_loss: 0.1223 - val_dense_4_loss: 0.1349 - val_dense_5_loss: 0.1609 - val_dense_6_loss: 0.1809 - val_dense_7_loss: 0.2488 - val_dense_8_loss: 0.2222 - val_dense_2_acc: 0.9699 - val_dense_3_acc: 0.9669 - val_dense_4_acc: 0.9657 - val_dense_5_acc: 0.9610 - val_dense_6_acc: 0.9579 - val_dense_7_acc: 0.9334 - val_dense_8_acc: 0.9379\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.23641 to 1.18323, saving model to seqence_wights.h5\n",
            "Epoch 43/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 2.0612 - dense_2_loss: 0.2690 - dense_3_loss: 0.2729 - dense_4_loss: 0.2867 - dense_5_loss: 0.3157 - dense_6_loss: 0.3270 - dense_7_loss: 0.3424 - dense_8_loss: 0.2475 - dense_2_acc: 0.9152 - dense_3_acc: 0.9141 - dense_4_acc: 0.9107 - dense_5_acc: 0.9013 - dense_6_acc: 0.8976 - dense_7_acc: 0.8940 - dense_8_acc: 0.9270 - val_loss: 1.1297 - val_dense_2_loss: 0.1057 - val_dense_3_loss: 0.1124 - val_dense_4_loss: 0.1297 - val_dense_5_loss: 0.1547 - val_dense_6_loss: 0.1747 - val_dense_7_loss: 0.2383 - val_dense_8_loss: 0.2143 - val_dense_2_acc: 0.9735 - val_dense_3_acc: 0.9714 - val_dense_4_acc: 0.9657 - val_dense_5_acc: 0.9623 - val_dense_6_acc: 0.9604 - val_dense_7_acc: 0.9373 - val_dense_8_acc: 0.9398\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.18323 to 1.12970, saving model to seqence_wights.h5\n",
            "Epoch 44/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.9964 - dense_2_loss: 0.2585 - dense_3_loss: 0.2673 - dense_4_loss: 0.2788 - dense_5_loss: 0.3039 - dense_6_loss: 0.3185 - dense_7_loss: 0.3285 - dense_8_loss: 0.2409 - dense_2_acc: 0.9185 - dense_3_acc: 0.9161 - dense_4_acc: 0.9125 - dense_5_acc: 0.9060 - dense_6_acc: 0.9000 - dense_7_acc: 0.8990 - dense_8_acc: 0.9283 - val_loss: 1.0680 - val_dense_2_loss: 0.0991 - val_dense_3_loss: 0.1065 - val_dense_4_loss: 0.1217 - val_dense_5_loss: 0.1444 - val_dense_6_loss: 0.1643 - val_dense_7_loss: 0.2266 - val_dense_8_loss: 0.2053 - val_dense_2_acc: 0.9741 - val_dense_3_acc: 0.9718 - val_dense_4_acc: 0.9686 - val_dense_5_acc: 0.9659 - val_dense_6_acc: 0.9603 - val_dense_7_acc: 0.9398 - val_dense_8_acc: 0.9409\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.12970 to 1.06800, saving model to seqence_wights.h5\n",
            "Epoch 45/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.9444 - dense_2_loss: 0.2519 - dense_3_loss: 0.2603 - dense_4_loss: 0.2729 - dense_5_loss: 0.2973 - dense_6_loss: 0.3083 - dense_7_loss: 0.3193 - dense_8_loss: 0.2344 - dense_2_acc: 0.9214 - dense_3_acc: 0.9181 - dense_4_acc: 0.9130 - dense_5_acc: 0.9059 - dense_6_acc: 0.9019 - dense_7_acc: 0.9012 - dense_8_acc: 0.9306 - val_loss: 1.0471 - val_dense_2_loss: 0.1001 - val_dense_3_loss: 0.1059 - val_dense_4_loss: 0.1194 - val_dense_5_loss: 0.1412 - val_dense_6_loss: 0.1609 - val_dense_7_loss: 0.2194 - val_dense_8_loss: 0.2002 - val_dense_2_acc: 0.9739 - val_dense_3_acc: 0.9727 - val_dense_4_acc: 0.9697 - val_dense_5_acc: 0.9680 - val_dense_6_acc: 0.9630 - val_dense_7_acc: 0.9435 - val_dense_8_acc: 0.9447\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.06800 to 1.04713, saving model to seqence_wights.h5\n",
            "Epoch 46/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.8974 - dense_2_loss: 0.2462 - dense_3_loss: 0.2549 - dense_4_loss: 0.2693 - dense_5_loss: 0.2909 - dense_6_loss: 0.2989 - dense_7_loss: 0.3117 - dense_8_loss: 0.2255 - dense_2_acc: 0.9228 - dense_3_acc: 0.9192 - dense_4_acc: 0.9143 - dense_5_acc: 0.9071 - dense_6_acc: 0.9055 - dense_7_acc: 0.9036 - dense_8_acc: 0.9325 - val_loss: 1.0189 - val_dense_2_loss: 0.0970 - val_dense_3_loss: 0.1037 - val_dense_4_loss: 0.1164 - val_dense_5_loss: 0.1378 - val_dense_6_loss: 0.1550 - val_dense_7_loss: 0.2134 - val_dense_8_loss: 0.1956 - val_dense_2_acc: 0.9751 - val_dense_3_acc: 0.9738 - val_dense_4_acc: 0.9710 - val_dense_5_acc: 0.9686 - val_dense_6_acc: 0.9657 - val_dense_7_acc: 0.9465 - val_dense_8_acc: 0.9466\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.04713 to 1.01891, saving model to seqence_wights.h5\n",
            "Epoch 47/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.8431 - dense_2_loss: 0.2373 - dense_3_loss: 0.2506 - dense_4_loss: 0.2640 - dense_5_loss: 0.2812 - dense_6_loss: 0.2878 - dense_7_loss: 0.2994 - dense_8_loss: 0.2228 - dense_2_acc: 0.9248 - dense_3_acc: 0.9211 - dense_4_acc: 0.9163 - dense_5_acc: 0.9116 - dense_6_acc: 0.9085 - dense_7_acc: 0.9067 - dense_8_acc: 0.9334 - val_loss: 0.9728 - val_dense_2_loss: 0.0915 - val_dense_3_loss: 0.0939 - val_dense_4_loss: 0.1074 - val_dense_5_loss: 0.1309 - val_dense_6_loss: 0.1501 - val_dense_7_loss: 0.2041 - val_dense_8_loss: 0.1949 - val_dense_2_acc: 0.9760 - val_dense_3_acc: 0.9755 - val_dense_4_acc: 0.9719 - val_dense_5_acc: 0.9681 - val_dense_6_acc: 0.9629 - val_dense_7_acc: 0.9457 - val_dense_8_acc: 0.9446\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.01891 to 0.97278, saving model to seqence_wights.h5\n",
            "Epoch 48/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 1.7985 - dense_2_loss: 0.2317 - dense_3_loss: 0.2438 - dense_4_loss: 0.2567 - dense_5_loss: 0.2756 - dense_6_loss: 0.2801 - dense_7_loss: 0.2923 - dense_8_loss: 0.2183 - dense_2_acc: 0.9265 - dense_3_acc: 0.9226 - dense_4_acc: 0.9182 - dense_5_acc: 0.9127 - dense_6_acc: 0.9119 - dense_7_acc: 0.9099 - dense_8_acc: 0.9349 - val_loss: 0.9124 - val_dense_2_loss: 0.0835 - val_dense_3_loss: 0.0910 - val_dense_4_loss: 0.1019 - val_dense_5_loss: 0.1208 - val_dense_6_loss: 0.1380 - val_dense_7_loss: 0.1936 - val_dense_8_loss: 0.1836 - val_dense_2_acc: 0.9784 - val_dense_3_acc: 0.9766 - val_dense_4_acc: 0.9728 - val_dense_5_acc: 0.9718 - val_dense_6_acc: 0.9671 - val_dense_7_acc: 0.9493 - val_dense_8_acc: 0.9481\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.97278 to 0.91238, saving model to seqence_wights.h5\n",
            "Epoch 49/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 1.7483 - dense_2_loss: 0.2271 - dense_3_loss: 0.2381 - dense_4_loss: 0.2490 - dense_5_loss: 0.2649 - dense_6_loss: 0.2732 - dense_7_loss: 0.2844 - dense_8_loss: 0.2117 - dense_2_acc: 0.9280 - dense_3_acc: 0.9240 - dense_4_acc: 0.9223 - dense_5_acc: 0.9158 - dense_6_acc: 0.9127 - dense_7_acc: 0.9114 - dense_8_acc: 0.9366 - val_loss: 0.8936 - val_dense_2_loss: 0.0807 - val_dense_3_loss: 0.0898 - val_dense_4_loss: 0.1008 - val_dense_5_loss: 0.1210 - val_dense_6_loss: 0.1363 - val_dense_7_loss: 0.1869 - val_dense_8_loss: 0.1781 - val_dense_2_acc: 0.9790 - val_dense_3_acc: 0.9768 - val_dense_4_acc: 0.9737 - val_dense_5_acc: 0.9715 - val_dense_6_acc: 0.9679 - val_dense_7_acc: 0.9526 - val_dense_8_acc: 0.9496\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.91238 to 0.89362, saving model to seqence_wights.h5\n",
            "Epoch 50/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.7037 - dense_2_loss: 0.2215 - dense_3_loss: 0.2258 - dense_4_loss: 0.2430 - dense_5_loss: 0.2604 - dense_6_loss: 0.2684 - dense_7_loss: 0.2783 - dense_8_loss: 0.2063 - dense_2_acc: 0.9290 - dense_3_acc: 0.9298 - dense_4_acc: 0.9223 - dense_5_acc: 0.9161 - dense_6_acc: 0.9144 - dense_7_acc: 0.9128 - dense_8_acc: 0.9376 - val_loss: 0.8763 - val_dense_2_loss: 0.0800 - val_dense_3_loss: 0.0887 - val_dense_4_loss: 0.0995 - val_dense_5_loss: 0.1177 - val_dense_6_loss: 0.1364 - val_dense_7_loss: 0.1815 - val_dense_8_loss: 0.1724 - val_dense_2_acc: 0.9802 - val_dense_3_acc: 0.9769 - val_dense_4_acc: 0.9753 - val_dense_5_acc: 0.9727 - val_dense_6_acc: 0.9682 - val_dense_7_acc: 0.9538 - val_dense_8_acc: 0.9524\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.89362 to 0.87631, saving model to seqence_wights.h5\n",
            "Epoch 51/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.6796 - dense_2_loss: 0.2212 - dense_3_loss: 0.2264 - dense_4_loss: 0.2336 - dense_5_loss: 0.2560 - dense_6_loss: 0.2617 - dense_7_loss: 0.2730 - dense_8_loss: 0.2077 - dense_2_acc: 0.9296 - dense_3_acc: 0.9274 - dense_4_acc: 0.9248 - dense_5_acc: 0.9173 - dense_6_acc: 0.9164 - dense_7_acc: 0.9144 - dense_8_acc: 0.9372 - val_loss: 0.8195 - val_dense_2_loss: 0.0752 - val_dense_3_loss: 0.0803 - val_dense_4_loss: 0.0909 - val_dense_5_loss: 0.1109 - val_dense_6_loss: 0.1235 - val_dense_7_loss: 0.1717 - val_dense_8_loss: 0.1671 - val_dense_2_acc: 0.9808 - val_dense_3_acc: 0.9793 - val_dense_4_acc: 0.9760 - val_dense_5_acc: 0.9738 - val_dense_6_acc: 0.9714 - val_dense_7_acc: 0.9564 - val_dense_8_acc: 0.9517\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.87631 to 0.81952, saving model to seqence_wights.h5\n",
            "Epoch 52/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.6319 - dense_2_loss: 0.2139 - dense_3_loss: 0.2163 - dense_4_loss: 0.2331 - dense_5_loss: 0.2491 - dense_6_loss: 0.2572 - dense_7_loss: 0.2641 - dense_8_loss: 0.1982 - dense_2_acc: 0.9320 - dense_3_acc: 0.9314 - dense_4_acc: 0.9253 - dense_5_acc: 0.9208 - dense_6_acc: 0.9175 - dense_7_acc: 0.9167 - dense_8_acc: 0.9390 - val_loss: 0.8010 - val_dense_2_loss: 0.0738 - val_dense_3_loss: 0.0803 - val_dense_4_loss: 0.0905 - val_dense_5_loss: 0.1072 - val_dense_6_loss: 0.1203 - val_dense_7_loss: 0.1669 - val_dense_8_loss: 0.1620 - val_dense_2_acc: 0.9819 - val_dense_3_acc: 0.9789 - val_dense_4_acc: 0.9776 - val_dense_5_acc: 0.9757 - val_dense_6_acc: 0.9717 - val_dense_7_acc: 0.9574 - val_dense_8_acc: 0.9539\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.81952 to 0.80096, saving model to seqence_wights.h5\n",
            "Epoch 53/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.6061 - dense_2_loss: 0.2130 - dense_3_loss: 0.2187 - dense_4_loss: 0.2270 - dense_5_loss: 0.2466 - dense_6_loss: 0.2507 - dense_7_loss: 0.2574 - dense_8_loss: 0.1926 - dense_2_acc: 0.9319 - dense_3_acc: 0.9297 - dense_4_acc: 0.9279 - dense_5_acc: 0.9199 - dense_6_acc: 0.9189 - dense_7_acc: 0.9189 - dense_8_acc: 0.9412 - val_loss: 0.7667 - val_dense_2_loss: 0.0702 - val_dense_3_loss: 0.0752 - val_dense_4_loss: 0.0849 - val_dense_5_loss: 0.1022 - val_dense_6_loss: 0.1139 - val_dense_7_loss: 0.1609 - val_dense_8_loss: 0.1594 - val_dense_2_acc: 0.9819 - val_dense_3_acc: 0.9804 - val_dense_4_acc: 0.9785 - val_dense_5_acc: 0.9756 - val_dense_6_acc: 0.9745 - val_dense_7_acc: 0.9599 - val_dense_8_acc: 0.9557\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.80096 to 0.76675, saving model to seqence_wights.h5\n",
            "Epoch 54/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.5649 - dense_2_loss: 0.2047 - dense_3_loss: 0.2081 - dense_4_loss: 0.2244 - dense_5_loss: 0.2409 - dense_6_loss: 0.2439 - dense_7_loss: 0.2510 - dense_8_loss: 0.1918 - dense_2_acc: 0.9344 - dense_3_acc: 0.9335 - dense_4_acc: 0.9279 - dense_5_acc: 0.9225 - dense_6_acc: 0.9219 - dense_7_acc: 0.9214 - dense_8_acc: 0.9409 - val_loss: 0.7473 - val_dense_2_loss: 0.0673 - val_dense_3_loss: 0.0765 - val_dense_4_loss: 0.0849 - val_dense_5_loss: 0.0972 - val_dense_6_loss: 0.1123 - val_dense_7_loss: 0.1542 - val_dense_8_loss: 0.1549 - val_dense_2_acc: 0.9830 - val_dense_3_acc: 0.9793 - val_dense_4_acc: 0.9780 - val_dense_5_acc: 0.9773 - val_dense_6_acc: 0.9739 - val_dense_7_acc: 0.9625 - val_dense_8_acc: 0.9560\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.76675 to 0.74731, saving model to seqence_wights.h5\n",
            "Epoch 55/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.5336 - dense_2_loss: 0.1998 - dense_3_loss: 0.2107 - dense_4_loss: 0.2190 - dense_5_loss: 0.2319 - dense_6_loss: 0.2386 - dense_7_loss: 0.2494 - dense_8_loss: 0.1841 - dense_2_acc: 0.9356 - dense_3_acc: 0.9318 - dense_4_acc: 0.9297 - dense_5_acc: 0.9262 - dense_6_acc: 0.9216 - dense_7_acc: 0.9208 - dense_8_acc: 0.9445 - val_loss: 0.7195 - val_dense_2_loss: 0.0638 - val_dense_3_loss: 0.0699 - val_dense_4_loss: 0.0798 - val_dense_5_loss: 0.0958 - val_dense_6_loss: 0.1101 - val_dense_7_loss: 0.1490 - val_dense_8_loss: 0.1512 - val_dense_2_acc: 0.9839 - val_dense_3_acc: 0.9822 - val_dense_4_acc: 0.9801 - val_dense_5_acc: 0.9776 - val_dense_6_acc: 0.9741 - val_dense_7_acc: 0.9637 - val_dense_8_acc: 0.9570\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.74731 to 0.71947, saving model to seqence_wights.h5\n",
            "Epoch 56/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 1.4911 - dense_2_loss: 0.2002 - dense_3_loss: 0.1958 - dense_4_loss: 0.2118 - dense_5_loss: 0.2333 - dense_6_loss: 0.2302 - dense_7_loss: 0.2393 - dense_8_loss: 0.1804 - dense_2_acc: 0.9358 - dense_3_acc: 0.9381 - dense_4_acc: 0.9316 - dense_5_acc: 0.9246 - dense_6_acc: 0.9251 - dense_7_acc: 0.9246 - dense_8_acc: 0.9442 - val_loss: 0.7033 - val_dense_2_loss: 0.0639 - val_dense_3_loss: 0.0708 - val_dense_4_loss: 0.0786 - val_dense_5_loss: 0.0946 - val_dense_6_loss: 0.1061 - val_dense_7_loss: 0.1432 - val_dense_8_loss: 0.1461 - val_dense_2_acc: 0.9834 - val_dense_3_acc: 0.9819 - val_dense_4_acc: 0.9799 - val_dense_5_acc: 0.9777 - val_dense_6_acc: 0.9759 - val_dense_7_acc: 0.9643 - val_dense_8_acc: 0.9586\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.71947 to 0.70334, saving model to seqence_wights.h5\n",
            "Epoch 57/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 1.4654 - dense_2_loss: 0.1923 - dense_3_loss: 0.2007 - dense_4_loss: 0.2124 - dense_5_loss: 0.2230 - dense_6_loss: 0.2288 - dense_7_loss: 0.2343 - dense_8_loss: 0.1739 - dense_2_acc: 0.9381 - dense_3_acc: 0.9339 - dense_4_acc: 0.9318 - dense_5_acc: 0.9283 - dense_6_acc: 0.9256 - dense_7_acc: 0.9252 - dense_8_acc: 0.9465 - val_loss: 0.6667 - val_dense_2_loss: 0.0594 - val_dense_3_loss: 0.0655 - val_dense_4_loss: 0.0738 - val_dense_5_loss: 0.0871 - val_dense_6_loss: 0.0999 - val_dense_7_loss: 0.1371 - val_dense_8_loss: 0.1437 - val_dense_2_acc: 0.9844 - val_dense_3_acc: 0.9825 - val_dense_4_acc: 0.9804 - val_dense_5_acc: 0.9792 - val_dense_6_acc: 0.9770 - val_dense_7_acc: 0.9670 - val_dense_8_acc: 0.9598\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.70334 to 0.66669, saving model to seqence_wights.h5\n",
            "Epoch 58/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 1.4466 - dense_2_loss: 0.1904 - dense_3_loss: 0.1925 - dense_4_loss: 0.2083 - dense_5_loss: 0.2225 - dense_6_loss: 0.2282 - dense_7_loss: 0.2276 - dense_8_loss: 0.1771 - dense_2_acc: 0.9388 - dense_3_acc: 0.9383 - dense_4_acc: 0.9330 - dense_5_acc: 0.9286 - dense_6_acc: 0.9251 - dense_7_acc: 0.9284 - dense_8_acc: 0.9457 - val_loss: 0.6471 - val_dense_2_loss: 0.0588 - val_dense_3_loss: 0.0631 - val_dense_4_loss: 0.0704 - val_dense_5_loss: 0.0876 - val_dense_6_loss: 0.0966 - val_dense_7_loss: 0.1342 - val_dense_8_loss: 0.1365 - val_dense_2_acc: 0.9842 - val_dense_3_acc: 0.9840 - val_dense_4_acc: 0.9813 - val_dense_5_acc: 0.9788 - val_dense_6_acc: 0.9774 - val_dense_7_acc: 0.9671 - val_dense_8_acc: 0.9615\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.66669 to 0.64713, saving model to seqence_wights.h5\n",
            "Epoch 59/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.4006 - dense_2_loss: 0.1832 - dense_3_loss: 0.1895 - dense_4_loss: 0.2005 - dense_5_loss: 0.2163 - dense_6_loss: 0.2162 - dense_7_loss: 0.2234 - dense_8_loss: 0.1716 - dense_2_acc: 0.9416 - dense_3_acc: 0.9391 - dense_4_acc: 0.9355 - dense_5_acc: 0.9304 - dense_6_acc: 0.9305 - dense_7_acc: 0.9287 - dense_8_acc: 0.9470 - val_loss: 0.6373 - val_dense_2_loss: 0.0574 - val_dense_3_loss: 0.0642 - val_dense_4_loss: 0.0696 - val_dense_5_loss: 0.0822 - val_dense_6_loss: 0.0940 - val_dense_7_loss: 0.1335 - val_dense_8_loss: 0.1365 - val_dense_2_acc: 0.9859 - val_dense_3_acc: 0.9843 - val_dense_4_acc: 0.9822 - val_dense_5_acc: 0.9804 - val_dense_6_acc: 0.9788 - val_dense_7_acc: 0.9657 - val_dense_8_acc: 0.9615\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.64713 to 0.63729, saving model to seqence_wights.h5\n",
            "Epoch 60/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 1.3804 - dense_2_loss: 0.1839 - dense_3_loss: 0.1855 - dense_4_loss: 0.1969 - dense_5_loss: 0.2140 - dense_6_loss: 0.2138 - dense_7_loss: 0.2205 - dense_8_loss: 0.1659 - dense_2_acc: 0.9408 - dense_3_acc: 0.9387 - dense_4_acc: 0.9375 - dense_5_acc: 0.9312 - dense_6_acc: 0.9308 - dense_7_acc: 0.9291 - dense_8_acc: 0.9487 - val_loss: 0.6098 - val_dense_2_loss: 0.0559 - val_dense_3_loss: 0.0602 - val_dense_4_loss: 0.0645 - val_dense_5_loss: 0.0773 - val_dense_6_loss: 0.0905 - val_dense_7_loss: 0.1262 - val_dense_8_loss: 0.1351 - val_dense_2_acc: 0.9850 - val_dense_3_acc: 0.9846 - val_dense_4_acc: 0.9831 - val_dense_5_acc: 0.9819 - val_dense_6_acc: 0.9787 - val_dense_7_acc: 0.9686 - val_dense_8_acc: 0.9614\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.63729 to 0.60979, saving model to seqence_wights.h5\n",
            "Epoch 61/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.3432 - dense_2_loss: 0.1761 - dense_3_loss: 0.1799 - dense_4_loss: 0.1903 - dense_5_loss: 0.2060 - dense_6_loss: 0.2113 - dense_7_loss: 0.2145 - dense_8_loss: 0.1650 - dense_2_acc: 0.9429 - dense_3_acc: 0.9425 - dense_4_acc: 0.9380 - dense_5_acc: 0.9335 - dense_6_acc: 0.9321 - dense_7_acc: 0.9323 - dense_8_acc: 0.9494 - val_loss: 0.5916 - val_dense_2_loss: 0.0524 - val_dense_3_loss: 0.0586 - val_dense_4_loss: 0.0652 - val_dense_5_loss: 0.0769 - val_dense_6_loss: 0.0883 - val_dense_7_loss: 0.1200 - val_dense_8_loss: 0.1302 - val_dense_2_acc: 0.9865 - val_dense_3_acc: 0.9854 - val_dense_4_acc: 0.9828 - val_dense_5_acc: 0.9821 - val_dense_6_acc: 0.9799 - val_dense_7_acc: 0.9716 - val_dense_8_acc: 0.9633\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.60979 to 0.59159, saving model to seqence_wights.h5\n",
            "Epoch 62/120\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 1.3242 - dense_2_loss: 0.1747 - dense_3_loss: 0.1770 - dense_4_loss: 0.1914 - dense_5_loss: 0.2071 - dense_6_loss: 0.2044 - dense_7_loss: 0.2084 - dense_8_loss: 0.1612 - dense_2_acc: 0.9449 - dense_3_acc: 0.9430 - dense_4_acc: 0.9381 - dense_5_acc: 0.9329 - dense_6_acc: 0.9334 - dense_7_acc: 0.9323 - dense_8_acc: 0.9505 - val_loss: 0.5651 - val_dense_2_loss: 0.0504 - val_dense_3_loss: 0.0539 - val_dense_4_loss: 0.0616 - val_dense_5_loss: 0.0730 - val_dense_6_loss: 0.0852 - val_dense_7_loss: 0.1168 - val_dense_8_loss: 0.1243 - val_dense_2_acc: 0.9877 - val_dense_3_acc: 0.9856 - val_dense_4_acc: 0.9849 - val_dense_5_acc: 0.9823 - val_dense_6_acc: 0.9802 - val_dense_7_acc: 0.9714 - val_dense_8_acc: 0.9647\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.59159 to 0.56514, saving model to seqence_wights.h5\n",
            "Epoch 63/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 1.2934 - dense_2_loss: 0.1717 - dense_3_loss: 0.1742 - dense_4_loss: 0.1857 - dense_5_loss: 0.1990 - dense_6_loss: 0.2023 - dense_7_loss: 0.2050 - dense_8_loss: 0.1555 - dense_2_acc: 0.9448 - dense_3_acc: 0.9427 - dense_4_acc: 0.9396 - dense_5_acc: 0.9353 - dense_6_acc: 0.9342 - dense_7_acc: 0.9348 - dense_8_acc: 0.9511 - val_loss: 0.5541 - val_dense_2_loss: 0.0493 - val_dense_3_loss: 0.0537 - val_dense_4_loss: 0.0619 - val_dense_5_loss: 0.0716 - val_dense_6_loss: 0.0816 - val_dense_7_loss: 0.1118 - val_dense_8_loss: 0.1243 - val_dense_2_acc: 0.9872 - val_dense_3_acc: 0.9866 - val_dense_4_acc: 0.9838 - val_dense_5_acc: 0.9836 - val_dense_6_acc: 0.9808 - val_dense_7_acc: 0.9734 - val_dense_8_acc: 0.9657\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.56514 to 0.55411, saving model to seqence_wights.h5\n",
            "Epoch 64/120\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 1.2717 - dense_2_loss: 0.1687 - dense_3_loss: 0.1728 - dense_4_loss: 0.1802 - dense_5_loss: 0.1926 - dense_6_loss: 0.2017 - dense_7_loss: 0.1984 - dense_8_loss: 0.1573 - dense_2_acc: 0.9453 - dense_3_acc: 0.9434 - dense_4_acc: 0.9405 - dense_5_acc: 0.9375 - dense_6_acc: 0.9346 - dense_7_acc: 0.9356 - dense_8_acc: 0.9504 - val_loss: 0.5312 - val_dense_2_loss: 0.0477 - val_dense_3_loss: 0.0535 - val_dense_4_loss: 0.0580 - val_dense_5_loss: 0.0696 - val_dense_6_loss: 0.0787 - val_dense_7_loss: 0.1055 - val_dense_8_loss: 0.1181 - val_dense_2_acc: 0.9872 - val_dense_3_acc: 0.9860 - val_dense_4_acc: 0.9850 - val_dense_5_acc: 0.9829 - val_dense_6_acc: 0.9807 - val_dense_7_acc: 0.9742 - val_dense_8_acc: 0.9662\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.55411 to 0.53123, saving model to seqence_wights.h5\n",
            "Epoch 65/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.2588 - dense_2_loss: 0.1675 - dense_3_loss: 0.1712 - dense_4_loss: 0.1797 - dense_5_loss: 0.1961 - dense_6_loss: 0.1947 - dense_7_loss: 0.1983 - dense_8_loss: 0.1513 - dense_2_acc: 0.9453 - dense_3_acc: 0.9436 - dense_4_acc: 0.9417 - dense_5_acc: 0.9362 - dense_6_acc: 0.9365 - dense_7_acc: 0.9368 - dense_8_acc: 0.9536 - val_loss: 0.5311 - val_dense_2_loss: 0.0466 - val_dense_3_loss: 0.0523 - val_dense_4_loss: 0.0587 - val_dense_5_loss: 0.0665 - val_dense_6_loss: 0.0818 - val_dense_7_loss: 0.1060 - val_dense_8_loss: 0.1191 - val_dense_2_acc: 0.9879 - val_dense_3_acc: 0.9868 - val_dense_4_acc: 0.9846 - val_dense_5_acc: 0.9845 - val_dense_6_acc: 0.9798 - val_dense_7_acc: 0.9738 - val_dense_8_acc: 0.9660\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.53123 to 0.53111, saving model to seqence_wights.h5\n",
            "Epoch 66/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 1.2301 - dense_2_loss: 0.1660 - dense_3_loss: 0.1653 - dense_4_loss: 0.1765 - dense_5_loss: 0.1893 - dense_6_loss: 0.1903 - dense_7_loss: 0.1920 - dense_8_loss: 0.1505 - dense_2_acc: 0.9460 - dense_3_acc: 0.9462 - dense_4_acc: 0.9429 - dense_5_acc: 0.9393 - dense_6_acc: 0.9391 - dense_7_acc: 0.9391 - dense_8_acc: 0.9531 - val_loss: 0.5049 - val_dense_2_loss: 0.0457 - val_dense_3_loss: 0.0485 - val_dense_4_loss: 0.0530 - val_dense_5_loss: 0.0637 - val_dense_6_loss: 0.0743 - val_dense_7_loss: 0.1040 - val_dense_8_loss: 0.1157 - val_dense_2_acc: 0.9880 - val_dense_3_acc: 0.9878 - val_dense_4_acc: 0.9859 - val_dense_5_acc: 0.9845 - val_dense_6_acc: 0.9834 - val_dense_7_acc: 0.9743 - val_dense_8_acc: 0.9673\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.53111 to 0.50493, saving model to seqence_wights.h5\n",
            "Epoch 67/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.1990 - dense_2_loss: 0.1609 - dense_3_loss: 0.1610 - dense_4_loss: 0.1722 - dense_5_loss: 0.1847 - dense_6_loss: 0.1878 - dense_7_loss: 0.1875 - dense_8_loss: 0.1448 - dense_2_acc: 0.9480 - dense_3_acc: 0.9465 - dense_4_acc: 0.9433 - dense_5_acc: 0.9405 - dense_6_acc: 0.9389 - dense_7_acc: 0.9394 - dense_8_acc: 0.9549 - val_loss: 0.4891 - val_dense_2_loss: 0.0430 - val_dense_3_loss: 0.0481 - val_dense_4_loss: 0.0532 - val_dense_5_loss: 0.0630 - val_dense_6_loss: 0.0724 - val_dense_7_loss: 0.0975 - val_dense_8_loss: 0.1118 - val_dense_2_acc: 0.9888 - val_dense_3_acc: 0.9882 - val_dense_4_acc: 0.9869 - val_dense_5_acc: 0.9850 - val_dense_6_acc: 0.9833 - val_dense_7_acc: 0.9770 - val_dense_8_acc: 0.9687\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.50493 to 0.48907, saving model to seqence_wights.h5\n",
            "Epoch 68/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 1.1872 - dense_2_loss: 0.1569 - dense_3_loss: 0.1637 - dense_4_loss: 0.1697 - dense_5_loss: 0.1810 - dense_6_loss: 0.1864 - dense_7_loss: 0.1874 - dense_8_loss: 0.1420 - dense_2_acc: 0.9489 - dense_3_acc: 0.9467 - dense_4_acc: 0.9445 - dense_5_acc: 0.9410 - dense_6_acc: 0.9389 - dense_7_acc: 0.9395 - dense_8_acc: 0.9547 - val_loss: 0.4761 - val_dense_2_loss: 0.0419 - val_dense_3_loss: 0.0467 - val_dense_4_loss: 0.0516 - val_dense_5_loss: 0.0601 - val_dense_6_loss: 0.0717 - val_dense_7_loss: 0.0957 - val_dense_8_loss: 0.1084 - val_dense_2_acc: 0.9892 - val_dense_3_acc: 0.9881 - val_dense_4_acc: 0.9869 - val_dense_5_acc: 0.9858 - val_dense_6_acc: 0.9830 - val_dense_7_acc: 0.9778 - val_dense_8_acc: 0.9705\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.48907 to 0.47607, saving model to seqence_wights.h5\n",
            "Epoch 69/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 1.1706 - dense_2_loss: 0.1553 - dense_3_loss: 0.1620 - dense_4_loss: 0.1665 - dense_5_loss: 0.1787 - dense_6_loss: 0.1841 - dense_7_loss: 0.1829 - dense_8_loss: 0.1411 - dense_2_acc: 0.9491 - dense_3_acc: 0.9481 - dense_4_acc: 0.9453 - dense_5_acc: 0.9416 - dense_6_acc: 0.9405 - dense_7_acc: 0.9414 - dense_8_acc: 0.9551 - val_loss: 0.4658 - val_dense_2_loss: 0.0428 - val_dense_3_loss: 0.0466 - val_dense_4_loss: 0.0497 - val_dense_5_loss: 0.0583 - val_dense_6_loss: 0.0678 - val_dense_7_loss: 0.0930 - val_dense_8_loss: 0.1076 - val_dense_2_acc: 0.9883 - val_dense_3_acc: 0.9877 - val_dense_4_acc: 0.9867 - val_dense_5_acc: 0.9861 - val_dense_6_acc: 0.9836 - val_dense_7_acc: 0.9777 - val_dense_8_acc: 0.9702\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.47607 to 0.46584, saving model to seqence_wights.h5\n",
            "Epoch 70/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 1.1437 - dense_2_loss: 0.1524 - dense_3_loss: 0.1549 - dense_4_loss: 0.1660 - dense_5_loss: 0.1761 - dense_6_loss: 0.1790 - dense_7_loss: 0.1783 - dense_8_loss: 0.1369 - dense_2_acc: 0.9504 - dense_3_acc: 0.9496 - dense_4_acc: 0.9459 - dense_5_acc: 0.9427 - dense_6_acc: 0.9423 - dense_7_acc: 0.9423 - dense_8_acc: 0.9573 - val_loss: 0.4494 - val_dense_2_loss: 0.0400 - val_dense_3_loss: 0.0442 - val_dense_4_loss: 0.0484 - val_dense_5_loss: 0.0573 - val_dense_6_loss: 0.0661 - val_dense_7_loss: 0.0900 - val_dense_8_loss: 0.1034 - val_dense_2_acc: 0.9899 - val_dense_3_acc: 0.9891 - val_dense_4_acc: 0.9880 - val_dense_5_acc: 0.9858 - val_dense_6_acc: 0.9846 - val_dense_7_acc: 0.9787 - val_dense_8_acc: 0.9710\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.46584 to 0.44938, saving model to seqence_wights.h5\n",
            "Epoch 71/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 1.1234 - dense_2_loss: 0.1508 - dense_3_loss: 0.1538 - dense_4_loss: 0.1596 - dense_5_loss: 0.1724 - dense_6_loss: 0.1737 - dense_7_loss: 0.1774 - dense_8_loss: 0.1355 - dense_2_acc: 0.9510 - dense_3_acc: 0.9490 - dense_4_acc: 0.9477 - dense_5_acc: 0.9436 - dense_6_acc: 0.9423 - dense_7_acc: 0.9433 - dense_8_acc: 0.9570 - val_loss: 0.4329 - val_dense_2_loss: 0.0373 - val_dense_3_loss: 0.0434 - val_dense_4_loss: 0.0465 - val_dense_5_loss: 0.0549 - val_dense_6_loss: 0.0624 - val_dense_7_loss: 0.0876 - val_dense_8_loss: 0.1009 - val_dense_2_acc: 0.9899 - val_dense_3_acc: 0.9880 - val_dense_4_acc: 0.9881 - val_dense_5_acc: 0.9868 - val_dense_6_acc: 0.9851 - val_dense_7_acc: 0.9789 - val_dense_8_acc: 0.9714\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.44938 to 0.43293, saving model to seqence_wights.h5\n",
            "Epoch 72/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 1.1043 - dense_2_loss: 0.1438 - dense_3_loss: 0.1541 - dense_4_loss: 0.1582 - dense_5_loss: 0.1690 - dense_6_loss: 0.1755 - dense_7_loss: 0.1721 - dense_8_loss: 0.1316 - dense_2_acc: 0.9537 - dense_3_acc: 0.9503 - dense_4_acc: 0.9469 - dense_5_acc: 0.9465 - dense_6_acc: 0.9417 - dense_7_acc: 0.9440 - dense_8_acc: 0.9591 - val_loss: 0.4321 - val_dense_2_loss: 0.0384 - val_dense_3_loss: 0.0432 - val_dense_4_loss: 0.0470 - val_dense_5_loss: 0.0546 - val_dense_6_loss: 0.0655 - val_dense_7_loss: 0.0856 - val_dense_8_loss: 0.0977 - val_dense_2_acc: 0.9902 - val_dense_3_acc: 0.9887 - val_dense_4_acc: 0.9882 - val_dense_5_acc: 0.9871 - val_dense_6_acc: 0.9842 - val_dense_7_acc: 0.9806 - val_dense_8_acc: 0.9728\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.43293 to 0.43208, saving model to seqence_wights.h5\n",
            "Epoch 73/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 1.0906 - dense_2_loss: 0.1473 - dense_3_loss: 0.1485 - dense_4_loss: 0.1546 - dense_5_loss: 0.1712 - dense_6_loss: 0.1703 - dense_7_loss: 0.1676 - dense_8_loss: 0.1310 - dense_2_acc: 0.9518 - dense_3_acc: 0.9511 - dense_4_acc: 0.9492 - dense_5_acc: 0.9441 - dense_6_acc: 0.9428 - dense_7_acc: 0.9450 - dense_8_acc: 0.9582 - val_loss: 0.4332 - val_dense_2_loss: 0.0387 - val_dense_3_loss: 0.0415 - val_dense_4_loss: 0.0489 - val_dense_5_loss: 0.0528 - val_dense_6_loss: 0.0653 - val_dense_7_loss: 0.0871 - val_dense_8_loss: 0.0988 - val_dense_2_acc: 0.9899 - val_dense_3_acc: 0.9891 - val_dense_4_acc: 0.9880 - val_dense_5_acc: 0.9873 - val_dense_6_acc: 0.9841 - val_dense_7_acc: 0.9796 - val_dense_8_acc: 0.9726\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.43208\n",
            "Epoch 74/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.0696 - dense_2_loss: 0.1425 - dense_3_loss: 0.1431 - dense_4_loss: 0.1551 - dense_5_loss: 0.1635 - dense_6_loss: 0.1670 - dense_7_loss: 0.1679 - dense_8_loss: 0.1307 - dense_2_acc: 0.9532 - dense_3_acc: 0.9533 - dense_4_acc: 0.9497 - dense_5_acc: 0.9467 - dense_6_acc: 0.9464 - dense_7_acc: 0.9448 - dense_8_acc: 0.9586 - val_loss: 0.4172 - val_dense_2_loss: 0.0363 - val_dense_3_loss: 0.0403 - val_dense_4_loss: 0.0444 - val_dense_5_loss: 0.0534 - val_dense_6_loss: 0.0640 - val_dense_7_loss: 0.0806 - val_dense_8_loss: 0.0981 - val_dense_2_acc: 0.9910 - val_dense_3_acc: 0.9897 - val_dense_4_acc: 0.9891 - val_dense_5_acc: 0.9865 - val_dense_6_acc: 0.9843 - val_dense_7_acc: 0.9821 - val_dense_8_acc: 0.9720\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.43208 to 0.41717, saving model to seqence_wights.h5\n",
            "Epoch 75/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 1.0537 - dense_2_loss: 0.1420 - dense_3_loss: 0.1425 - dense_4_loss: 0.1510 - dense_5_loss: 0.1625 - dense_6_loss: 0.1645 - dense_7_loss: 0.1626 - dense_8_loss: 0.1286 - dense_2_acc: 0.9534 - dense_3_acc: 0.9527 - dense_4_acc: 0.9499 - dense_5_acc: 0.9474 - dense_6_acc: 0.9454 - dense_7_acc: 0.9474 - dense_8_acc: 0.9593 - val_loss: 0.3971 - val_dense_2_loss: 0.0337 - val_dense_3_loss: 0.0384 - val_dense_4_loss: 0.0418 - val_dense_5_loss: 0.0495 - val_dense_6_loss: 0.0589 - val_dense_7_loss: 0.0789 - val_dense_8_loss: 0.0958 - val_dense_2_acc: 0.9906 - val_dense_3_acc: 0.9899 - val_dense_4_acc: 0.9893 - val_dense_5_acc: 0.9880 - val_dense_6_acc: 0.9859 - val_dense_7_acc: 0.9815 - val_dense_8_acc: 0.9720\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.41717 to 0.39710, saving model to seqence_wights.h5\n",
            "Epoch 76/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 1.0465 - dense_2_loss: 0.1408 - dense_3_loss: 0.1417 - dense_4_loss: 0.1519 - dense_5_loss: 0.1595 - dense_6_loss: 0.1638 - dense_7_loss: 0.1622 - dense_8_loss: 0.1266 - dense_2_acc: 0.9540 - dense_3_acc: 0.9538 - dense_4_acc: 0.9503 - dense_5_acc: 0.9485 - dense_6_acc: 0.9467 - dense_7_acc: 0.9479 - dense_8_acc: 0.9600 - val_loss: 0.3893 - val_dense_2_loss: 0.0330 - val_dense_3_loss: 0.0380 - val_dense_4_loss: 0.0409 - val_dense_5_loss: 0.0480 - val_dense_6_loss: 0.0588 - val_dense_7_loss: 0.0777 - val_dense_8_loss: 0.0929 - val_dense_2_acc: 0.9912 - val_dense_3_acc: 0.9902 - val_dense_4_acc: 0.9891 - val_dense_5_acc: 0.9885 - val_dense_6_acc: 0.9849 - val_dense_7_acc: 0.9808 - val_dense_8_acc: 0.9738\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.39710 to 0.38932, saving model to seqence_wights.h5\n",
            "Epoch 77/120\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 1.0222 - dense_2_loss: 0.1402 - dense_3_loss: 0.1387 - dense_4_loss: 0.1460 - dense_5_loss: 0.1584 - dense_6_loss: 0.1576 - dense_7_loss: 0.1588 - dense_8_loss: 0.1225 - dense_2_acc: 0.9546 - dense_3_acc: 0.9548 - dense_4_acc: 0.9517 - dense_5_acc: 0.9478 - dense_6_acc: 0.9483 - dense_7_acc: 0.9477 - dense_8_acc: 0.9610 - val_loss: 0.3705 - val_dense_2_loss: 0.0320 - val_dense_3_loss: 0.0367 - val_dense_4_loss: 0.0396 - val_dense_5_loss: 0.0459 - val_dense_6_loss: 0.0550 - val_dense_7_loss: 0.0721 - val_dense_8_loss: 0.0891 - val_dense_2_acc: 0.9910 - val_dense_3_acc: 0.9903 - val_dense_4_acc: 0.9899 - val_dense_5_acc: 0.9882 - val_dense_6_acc: 0.9865 - val_dense_7_acc: 0.9838 - val_dense_8_acc: 0.9740\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.38932 to 0.37050, saving model to seqence_wights.h5\n",
            "Epoch 78/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 1.0062 - dense_2_loss: 0.1340 - dense_3_loss: 0.1408 - dense_4_loss: 0.1459 - dense_5_loss: 0.1525 - dense_6_loss: 0.1571 - dense_7_loss: 0.1555 - dense_8_loss: 0.1205 - dense_2_acc: 0.9565 - dense_3_acc: 0.9547 - dense_4_acc: 0.9527 - dense_5_acc: 0.9491 - dense_6_acc: 0.9479 - dense_7_acc: 0.9497 - dense_8_acc: 0.9613 - val_loss: 0.3603 - val_dense_2_loss: 0.0316 - val_dense_3_loss: 0.0362 - val_dense_4_loss: 0.0384 - val_dense_5_loss: 0.0441 - val_dense_6_loss: 0.0538 - val_dense_7_loss: 0.0700 - val_dense_8_loss: 0.0862 - val_dense_2_acc: 0.9915 - val_dense_3_acc: 0.9903 - val_dense_4_acc: 0.9896 - val_dense_5_acc: 0.9891 - val_dense_6_acc: 0.9859 - val_dense_7_acc: 0.9839 - val_dense_8_acc: 0.9750\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.37050 to 0.36030, saving model to seqence_wights.h5\n",
            "Epoch 79/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.9991 - dense_2_loss: 0.1353 - dense_3_loss: 0.1404 - dense_4_loss: 0.1405 - dense_5_loss: 0.1519 - dense_6_loss: 0.1545 - dense_7_loss: 0.1560 - dense_8_loss: 0.1205 - dense_2_acc: 0.9548 - dense_3_acc: 0.9533 - dense_4_acc: 0.9538 - dense_5_acc: 0.9500 - dense_6_acc: 0.9495 - dense_7_acc: 0.9494 - dense_8_acc: 0.9624 - val_loss: 0.3674 - val_dense_2_loss: 0.0318 - val_dense_3_loss: 0.0379 - val_dense_4_loss: 0.0394 - val_dense_5_loss: 0.0465 - val_dense_6_loss: 0.0537 - val_dense_7_loss: 0.0721 - val_dense_8_loss: 0.0861 - val_dense_2_acc: 0.9920 - val_dense_3_acc: 0.9903 - val_dense_4_acc: 0.9899 - val_dense_5_acc: 0.9889 - val_dense_6_acc: 0.9872 - val_dense_7_acc: 0.9830 - val_dense_8_acc: 0.9760\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.36030\n",
            "Epoch 80/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.9737 - dense_2_loss: 0.1294 - dense_3_loss: 0.1347 - dense_4_loss: 0.1421 - dense_5_loss: 0.1492 - dense_6_loss: 0.1511 - dense_7_loss: 0.1492 - dense_8_loss: 0.1179 - dense_2_acc: 0.9571 - dense_3_acc: 0.9546 - dense_4_acc: 0.9535 - dense_5_acc: 0.9513 - dense_6_acc: 0.9490 - dense_7_acc: 0.9518 - dense_8_acc: 0.9618 - val_loss: 0.3527 - val_dense_2_loss: 0.0316 - val_dense_3_loss: 0.0350 - val_dense_4_loss: 0.0372 - val_dense_5_loss: 0.0442 - val_dense_6_loss: 0.0532 - val_dense_7_loss: 0.0681 - val_dense_8_loss: 0.0834 - val_dense_2_acc: 0.9912 - val_dense_3_acc: 0.9906 - val_dense_4_acc: 0.9907 - val_dense_5_acc: 0.9888 - val_dense_6_acc: 0.9871 - val_dense_7_acc: 0.9846 - val_dense_8_acc: 0.9762\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.36030 to 0.35273, saving model to seqence_wights.h5\n",
            "Epoch 81/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.9605 - dense_2_loss: 0.1289 - dense_3_loss: 0.1312 - dense_4_loss: 0.1365 - dense_5_loss: 0.1515 - dense_6_loss: 0.1480 - dense_7_loss: 0.1477 - dense_8_loss: 0.1168 - dense_2_acc: 0.9573 - dense_3_acc: 0.9573 - dense_4_acc: 0.9548 - dense_5_acc: 0.9498 - dense_6_acc: 0.9513 - dense_7_acc: 0.9515 - dense_8_acc: 0.9623 - val_loss: 0.3579 - val_dense_2_loss: 0.0319 - val_dense_3_loss: 0.0361 - val_dense_4_loss: 0.0383 - val_dense_5_loss: 0.0464 - val_dense_6_loss: 0.0525 - val_dense_7_loss: 0.0693 - val_dense_8_loss: 0.0834 - val_dense_2_acc: 0.9917 - val_dense_3_acc: 0.9906 - val_dense_4_acc: 0.9903 - val_dense_5_acc: 0.9891 - val_dense_6_acc: 0.9872 - val_dense_7_acc: 0.9849 - val_dense_8_acc: 0.9764\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.35273\n",
            "Epoch 82/120\n",
            "60000/60000 [==============================] - 11s 192us/step - loss: 0.9488 - dense_2_loss: 0.1265 - dense_3_loss: 0.1318 - dense_4_loss: 0.1363 - dense_5_loss: 0.1467 - dense_6_loss: 0.1473 - dense_7_loss: 0.1458 - dense_8_loss: 0.1144 - dense_2_acc: 0.9578 - dense_3_acc: 0.9559 - dense_4_acc: 0.9552 - dense_5_acc: 0.9518 - dense_6_acc: 0.9508 - dense_7_acc: 0.9513 - dense_8_acc: 0.9642 - val_loss: 0.3372 - val_dense_2_loss: 0.0307 - val_dense_3_loss: 0.0349 - val_dense_4_loss: 0.0349 - val_dense_5_loss: 0.0415 - val_dense_6_loss: 0.0496 - val_dense_7_loss: 0.0650 - val_dense_8_loss: 0.0807 - val_dense_2_acc: 0.9916 - val_dense_3_acc: 0.9912 - val_dense_4_acc: 0.9911 - val_dense_5_acc: 0.9894 - val_dense_6_acc: 0.9879 - val_dense_7_acc: 0.9855 - val_dense_8_acc: 0.9774\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.35273 to 0.33722, saving model to seqence_wights.h5\n",
            "Epoch 83/120\n",
            "60000/60000 [==============================] - 12s 192us/step - loss: 0.9395 - dense_2_loss: 0.1271 - dense_3_loss: 0.1327 - dense_4_loss: 0.1349 - dense_5_loss: 0.1444 - dense_6_loss: 0.1459 - dense_7_loss: 0.1439 - dense_8_loss: 0.1105 - dense_2_acc: 0.9574 - dense_3_acc: 0.9560 - dense_4_acc: 0.9552 - dense_5_acc: 0.9526 - dense_6_acc: 0.9513 - dense_7_acc: 0.9528 - dense_8_acc: 0.9647 - val_loss: 0.3295 - val_dense_2_loss: 0.0292 - val_dense_3_loss: 0.0325 - val_dense_4_loss: 0.0337 - val_dense_5_loss: 0.0389 - val_dense_6_loss: 0.0484 - val_dense_7_loss: 0.0638 - val_dense_8_loss: 0.0829 - val_dense_2_acc: 0.9921 - val_dense_3_acc: 0.9916 - val_dense_4_acc: 0.9913 - val_dense_5_acc: 0.9907 - val_dense_6_acc: 0.9878 - val_dense_7_acc: 0.9851 - val_dense_8_acc: 0.9758\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.33722 to 0.32952, saving model to seqence_wights.h5\n",
            "Epoch 84/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.9284 - dense_2_loss: 0.1256 - dense_3_loss: 0.1294 - dense_4_loss: 0.1308 - dense_5_loss: 0.1436 - dense_6_loss: 0.1448 - dense_7_loss: 0.1409 - dense_8_loss: 0.1133 - dense_2_acc: 0.9584 - dense_3_acc: 0.9568 - dense_4_acc: 0.9570 - dense_5_acc: 0.9534 - dense_6_acc: 0.9527 - dense_7_acc: 0.9543 - dense_8_acc: 0.9631 - val_loss: 0.3230 - val_dense_2_loss: 0.0275 - val_dense_3_loss: 0.0334 - val_dense_4_loss: 0.0333 - val_dense_5_loss: 0.0403 - val_dense_6_loss: 0.0479 - val_dense_7_loss: 0.0634 - val_dense_8_loss: 0.0772 - val_dense_2_acc: 0.9921 - val_dense_3_acc: 0.9913 - val_dense_4_acc: 0.9914 - val_dense_5_acc: 0.9899 - val_dense_6_acc: 0.9874 - val_dense_7_acc: 0.9850 - val_dense_8_acc: 0.9782\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.32952 to 0.32300, saving model to seqence_wights.h5\n",
            "Epoch 85/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.9078 - dense_2_loss: 0.1238 - dense_3_loss: 0.1231 - dense_4_loss: 0.1288 - dense_5_loss: 0.1402 - dense_6_loss: 0.1396 - dense_7_loss: 0.1405 - dense_8_loss: 0.1117 - dense_2_acc: 0.9587 - dense_3_acc: 0.9588 - dense_4_acc: 0.9563 - dense_5_acc: 0.9534 - dense_6_acc: 0.9538 - dense_7_acc: 0.9552 - dense_8_acc: 0.9641 - val_loss: 0.3165 - val_dense_2_loss: 0.0286 - val_dense_3_loss: 0.0333 - val_dense_4_loss: 0.0330 - val_dense_5_loss: 0.0384 - val_dense_6_loss: 0.0469 - val_dense_7_loss: 0.0597 - val_dense_8_loss: 0.0768 - val_dense_2_acc: 0.9923 - val_dense_3_acc: 0.9907 - val_dense_4_acc: 0.9917 - val_dense_5_acc: 0.9905 - val_dense_6_acc: 0.9884 - val_dense_7_acc: 0.9868 - val_dense_8_acc: 0.9782\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.32300 to 0.31651, saving model to seqence_wights.h5\n",
            "Epoch 86/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.8949 - dense_2_loss: 0.1233 - dense_3_loss: 0.1199 - dense_4_loss: 0.1283 - dense_5_loss: 0.1407 - dense_6_loss: 0.1392 - dense_7_loss: 0.1376 - dense_8_loss: 0.1060 - dense_2_acc: 0.9586 - dense_3_acc: 0.9611 - dense_4_acc: 0.9574 - dense_5_acc: 0.9540 - dense_6_acc: 0.9545 - dense_7_acc: 0.9548 - dense_8_acc: 0.9660 - val_loss: 0.3090 - val_dense_2_loss: 0.0263 - val_dense_3_loss: 0.0309 - val_dense_4_loss: 0.0309 - val_dense_5_loss: 0.0378 - val_dense_6_loss: 0.0470 - val_dense_7_loss: 0.0598 - val_dense_8_loss: 0.0765 - val_dense_2_acc: 0.9931 - val_dense_3_acc: 0.9915 - val_dense_4_acc: 0.9919 - val_dense_5_acc: 0.9904 - val_dense_6_acc: 0.9879 - val_dense_7_acc: 0.9853 - val_dense_8_acc: 0.9778\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.31651 to 0.30897, saving model to seqence_wights.h5\n",
            "Epoch 87/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.8856 - dense_2_loss: 0.1150 - dense_3_loss: 0.1245 - dense_4_loss: 0.1244 - dense_5_loss: 0.1365 - dense_6_loss: 0.1398 - dense_7_loss: 0.1388 - dense_8_loss: 0.1067 - dense_2_acc: 0.9621 - dense_3_acc: 0.9586 - dense_4_acc: 0.9581 - dense_5_acc: 0.9546 - dense_6_acc: 0.9533 - dense_7_acc: 0.9550 - dense_8_acc: 0.9659 - val_loss: 0.2973 - val_dense_2_loss: 0.0261 - val_dense_3_loss: 0.0311 - val_dense_4_loss: 0.0305 - val_dense_5_loss: 0.0367 - val_dense_6_loss: 0.0436 - val_dense_7_loss: 0.0575 - val_dense_8_loss: 0.0719 - val_dense_2_acc: 0.9929 - val_dense_3_acc: 0.9918 - val_dense_4_acc: 0.9925 - val_dense_5_acc: 0.9905 - val_dense_6_acc: 0.9884 - val_dense_7_acc: 0.9860 - val_dense_8_acc: 0.9806\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.30897 to 0.29733, saving model to seqence_wights.h5\n",
            "Epoch 88/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.8823 - dense_2_loss: 0.1187 - dense_3_loss: 0.1205 - dense_4_loss: 0.1275 - dense_5_loss: 0.1356 - dense_6_loss: 0.1378 - dense_7_loss: 0.1372 - dense_8_loss: 0.1049 - dense_2_acc: 0.9608 - dense_3_acc: 0.9599 - dense_4_acc: 0.9572 - dense_5_acc: 0.9551 - dense_6_acc: 0.9544 - dense_7_acc: 0.9538 - dense_8_acc: 0.9660 - val_loss: 0.3012 - val_dense_2_loss: 0.0266 - val_dense_3_loss: 0.0307 - val_dense_4_loss: 0.0303 - val_dense_5_loss: 0.0381 - val_dense_6_loss: 0.0437 - val_dense_7_loss: 0.0575 - val_dense_8_loss: 0.0743 - val_dense_2_acc: 0.9927 - val_dense_3_acc: 0.9916 - val_dense_4_acc: 0.9925 - val_dense_5_acc: 0.9904 - val_dense_6_acc: 0.9889 - val_dense_7_acc: 0.9864 - val_dense_8_acc: 0.9788\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.29733\n",
            "Epoch 89/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.8671 - dense_2_loss: 0.1201 - dense_3_loss: 0.1202 - dense_4_loss: 0.1229 - dense_5_loss: 0.1343 - dense_6_loss: 0.1347 - dense_7_loss: 0.1309 - dense_8_loss: 0.1040 - dense_2_acc: 0.9602 - dense_3_acc: 0.9608 - dense_4_acc: 0.9595 - dense_5_acc: 0.9559 - dense_6_acc: 0.9559 - dense_7_acc: 0.9570 - dense_8_acc: 0.9668 - val_loss: 0.2964 - val_dense_2_loss: 0.0244 - val_dense_3_loss: 0.0322 - val_dense_4_loss: 0.0300 - val_dense_5_loss: 0.0367 - val_dense_6_loss: 0.0452 - val_dense_7_loss: 0.0560 - val_dense_8_loss: 0.0720 - val_dense_2_acc: 0.9931 - val_dense_3_acc: 0.9903 - val_dense_4_acc: 0.9928 - val_dense_5_acc: 0.9910 - val_dense_6_acc: 0.9881 - val_dense_7_acc: 0.9874 - val_dense_8_acc: 0.9794\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.29733 to 0.29642, saving model to seqence_wights.h5\n",
            "Epoch 90/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.8527 - dense_2_loss: 0.1170 - dense_3_loss: 0.1167 - dense_4_loss: 0.1208 - dense_5_loss: 0.1305 - dense_6_loss: 0.1331 - dense_7_loss: 0.1329 - dense_8_loss: 0.1017 - dense_2_acc: 0.9611 - dense_3_acc: 0.9614 - dense_4_acc: 0.9596 - dense_5_acc: 0.9565 - dense_6_acc: 0.9571 - dense_7_acc: 0.9563 - dense_8_acc: 0.9673 - val_loss: 0.2888 - val_dense_2_loss: 0.0256 - val_dense_3_loss: 0.0295 - val_dense_4_loss: 0.0298 - val_dense_5_loss: 0.0348 - val_dense_6_loss: 0.0433 - val_dense_7_loss: 0.0548 - val_dense_8_loss: 0.0710 - val_dense_2_acc: 0.9934 - val_dense_3_acc: 0.9922 - val_dense_4_acc: 0.9930 - val_dense_5_acc: 0.9913 - val_dense_6_acc: 0.9892 - val_dense_7_acc: 0.9872 - val_dense_8_acc: 0.9805\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.29642 to 0.28883, saving model to seqence_wights.h5\n",
            "Epoch 91/120\n",
            "60000/60000 [==============================] - 12s 193us/step - loss: 0.8447 - dense_2_loss: 0.1134 - dense_3_loss: 0.1167 - dense_4_loss: 0.1198 - dense_5_loss: 0.1308 - dense_6_loss: 0.1300 - dense_7_loss: 0.1320 - dense_8_loss: 0.1020 - dense_2_acc: 0.9611 - dense_3_acc: 0.9616 - dense_4_acc: 0.9602 - dense_5_acc: 0.9566 - dense_6_acc: 0.9572 - dense_7_acc: 0.9562 - dense_8_acc: 0.9670 - val_loss: 0.2789 - val_dense_2_loss: 0.0241 - val_dense_3_loss: 0.0286 - val_dense_4_loss: 0.0288 - val_dense_5_loss: 0.0343 - val_dense_6_loss: 0.0411 - val_dense_7_loss: 0.0530 - val_dense_8_loss: 0.0688 - val_dense_2_acc: 0.9933 - val_dense_3_acc: 0.9919 - val_dense_4_acc: 0.9929 - val_dense_5_acc: 0.9917 - val_dense_6_acc: 0.9895 - val_dense_7_acc: 0.9870 - val_dense_8_acc: 0.9798\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.28883 to 0.27886, saving model to seqence_wights.h5\n",
            "Epoch 92/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.8296 - dense_2_loss: 0.1126 - dense_3_loss: 0.1165 - dense_4_loss: 0.1198 - dense_5_loss: 0.1275 - dense_6_loss: 0.1301 - dense_7_loss: 0.1245 - dense_8_loss: 0.0986 - dense_2_acc: 0.9628 - dense_3_acc: 0.9610 - dense_4_acc: 0.9602 - dense_5_acc: 0.9573 - dense_6_acc: 0.9565 - dense_7_acc: 0.9591 - dense_8_acc: 0.9681 - val_loss: 0.2792 - val_dense_2_loss: 0.0247 - val_dense_3_loss: 0.0287 - val_dense_4_loss: 0.0288 - val_dense_5_loss: 0.0343 - val_dense_6_loss: 0.0422 - val_dense_7_loss: 0.0529 - val_dense_8_loss: 0.0676 - val_dense_2_acc: 0.9935 - val_dense_3_acc: 0.9922 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9915 - val_dense_6_acc: 0.9895 - val_dense_7_acc: 0.9883 - val_dense_8_acc: 0.9814\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.27886\n",
            "Epoch 93/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.8278 - dense_2_loss: 0.1127 - dense_3_loss: 0.1142 - dense_4_loss: 0.1201 - dense_5_loss: 0.1274 - dense_6_loss: 0.1292 - dense_7_loss: 0.1261 - dense_8_loss: 0.0980 - dense_2_acc: 0.9628 - dense_3_acc: 0.9621 - dense_4_acc: 0.9608 - dense_5_acc: 0.9573 - dense_6_acc: 0.9572 - dense_7_acc: 0.9588 - dense_8_acc: 0.9679 - val_loss: 0.2678 - val_dense_2_loss: 0.0235 - val_dense_3_loss: 0.0287 - val_dense_4_loss: 0.0277 - val_dense_5_loss: 0.0320 - val_dense_6_loss: 0.0397 - val_dense_7_loss: 0.0509 - val_dense_8_loss: 0.0653 - val_dense_2_acc: 0.9932 - val_dense_3_acc: 0.9924 - val_dense_4_acc: 0.9926 - val_dense_5_acc: 0.9919 - val_dense_6_acc: 0.9898 - val_dense_7_acc: 0.9872 - val_dense_8_acc: 0.9814\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.27886 to 0.26784, saving model to seqence_wights.h5\n",
            "Epoch 94/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.8174 - dense_2_loss: 0.1122 - dense_3_loss: 0.1132 - dense_4_loss: 0.1176 - dense_5_loss: 0.1256 - dense_6_loss: 0.1273 - dense_7_loss: 0.1240 - dense_8_loss: 0.0974 - dense_2_acc: 0.9631 - dense_3_acc: 0.9628 - dense_4_acc: 0.9615 - dense_5_acc: 0.9577 - dense_6_acc: 0.9584 - dense_7_acc: 0.9588 - dense_8_acc: 0.9681 - val_loss: 0.2695 - val_dense_2_loss: 0.0231 - val_dense_3_loss: 0.0283 - val_dense_4_loss: 0.0276 - val_dense_5_loss: 0.0322 - val_dense_6_loss: 0.0399 - val_dense_7_loss: 0.0524 - val_dense_8_loss: 0.0660 - val_dense_2_acc: 0.9938 - val_dense_3_acc: 0.9922 - val_dense_4_acc: 0.9932 - val_dense_5_acc: 0.9915 - val_dense_6_acc: 0.9902 - val_dense_7_acc: 0.9872 - val_dense_8_acc: 0.9819\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.26784\n",
            "Epoch 95/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.8039 - dense_2_loss: 0.1088 - dense_3_loss: 0.1112 - dense_4_loss: 0.1171 - dense_5_loss: 0.1219 - dense_6_loss: 0.1249 - dense_7_loss: 0.1240 - dense_8_loss: 0.0958 - dense_2_acc: 0.9640 - dense_3_acc: 0.9622 - dense_4_acc: 0.9610 - dense_5_acc: 0.9588 - dense_6_acc: 0.9580 - dense_7_acc: 0.9591 - dense_8_acc: 0.9693 - val_loss: 0.2559 - val_dense_2_loss: 0.0220 - val_dense_3_loss: 0.0259 - val_dense_4_loss: 0.0255 - val_dense_5_loss: 0.0305 - val_dense_6_loss: 0.0382 - val_dense_7_loss: 0.0494 - val_dense_8_loss: 0.0644 - val_dense_2_acc: 0.9939 - val_dense_3_acc: 0.9930 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9920 - val_dense_6_acc: 0.9900 - val_dense_7_acc: 0.9882 - val_dense_8_acc: 0.9812\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.26784 to 0.25587, saving model to seqence_wights.h5\n",
            "Epoch 96/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.7864 - dense_2_loss: 0.1103 - dense_3_loss: 0.1094 - dense_4_loss: 0.1146 - dense_5_loss: 0.1221 - dense_6_loss: 0.1198 - dense_7_loss: 0.1196 - dense_8_loss: 0.0907 - dense_2_acc: 0.9636 - dense_3_acc: 0.9634 - dense_4_acc: 0.9622 - dense_5_acc: 0.9596 - dense_6_acc: 0.9603 - dense_7_acc: 0.9603 - dense_8_acc: 0.9704 - val_loss: 0.2668 - val_dense_2_loss: 0.0213 - val_dense_3_loss: 0.0275 - val_dense_4_loss: 0.0259 - val_dense_5_loss: 0.0328 - val_dense_6_loss: 0.0379 - val_dense_7_loss: 0.0484 - val_dense_8_loss: 0.0730 - val_dense_2_acc: 0.9943 - val_dense_3_acc: 0.9922 - val_dense_4_acc: 0.9929 - val_dense_5_acc: 0.9916 - val_dense_6_acc: 0.9902 - val_dense_7_acc: 0.9886 - val_dense_8_acc: 0.9782\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.25587\n",
            "Epoch 97/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.7888 - dense_2_loss: 0.1062 - dense_3_loss: 0.1103 - dense_4_loss: 0.1124 - dense_5_loss: 0.1219 - dense_6_loss: 0.1248 - dense_7_loss: 0.1183 - dense_8_loss: 0.0950 - dense_2_acc: 0.9646 - dense_3_acc: 0.9631 - dense_4_acc: 0.9628 - dense_5_acc: 0.9598 - dense_6_acc: 0.9587 - dense_7_acc: 0.9608 - dense_8_acc: 0.9694 - val_loss: 0.2574 - val_dense_2_loss: 0.0223 - val_dense_3_loss: 0.0257 - val_dense_4_loss: 0.0259 - val_dense_5_loss: 0.0310 - val_dense_6_loss: 0.0378 - val_dense_7_loss: 0.0497 - val_dense_8_loss: 0.0650 - val_dense_2_acc: 0.9938 - val_dense_3_acc: 0.9933 - val_dense_4_acc: 0.9936 - val_dense_5_acc: 0.9918 - val_dense_6_acc: 0.9905 - val_dense_7_acc: 0.9870 - val_dense_8_acc: 0.9811\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.25587\n",
            "Epoch 98/120\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.7734 - dense_2_loss: 0.1050 - dense_3_loss: 0.1092 - dense_4_loss: 0.1092 - dense_5_loss: 0.1167 - dense_6_loss: 0.1208 - dense_7_loss: 0.1185 - dense_8_loss: 0.0939 - dense_2_acc: 0.9646 - dense_3_acc: 0.9638 - dense_4_acc: 0.9637 - dense_5_acc: 0.9612 - dense_6_acc: 0.9606 - dense_7_acc: 0.9610 - dense_8_acc: 0.9696 - val_loss: 0.2489 - val_dense_2_loss: 0.0215 - val_dense_3_loss: 0.0256 - val_dense_4_loss: 0.0256 - val_dense_5_loss: 0.0312 - val_dense_6_loss: 0.0373 - val_dense_7_loss: 0.0455 - val_dense_8_loss: 0.0622 - val_dense_2_acc: 0.9942 - val_dense_3_acc: 0.9933 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9916 - val_dense_6_acc: 0.9906 - val_dense_7_acc: 0.9899 - val_dense_8_acc: 0.9826\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.25587 to 0.24885, saving model to seqence_wights.h5\n",
            "Epoch 99/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.7648 - dense_2_loss: 0.1060 - dense_3_loss: 0.1058 - dense_4_loss: 0.1112 - dense_5_loss: 0.1178 - dense_6_loss: 0.1171 - dense_7_loss: 0.1161 - dense_8_loss: 0.0907 - dense_2_acc: 0.9648 - dense_3_acc: 0.9642 - dense_4_acc: 0.9628 - dense_5_acc: 0.9604 - dense_6_acc: 0.9609 - dense_7_acc: 0.9616 - dense_8_acc: 0.9706 - val_loss: 0.2409 - val_dense_2_loss: 0.0206 - val_dense_3_loss: 0.0259 - val_dense_4_loss: 0.0251 - val_dense_5_loss: 0.0287 - val_dense_6_loss: 0.0360 - val_dense_7_loss: 0.0435 - val_dense_8_loss: 0.0611 - val_dense_2_acc: 0.9943 - val_dense_3_acc: 0.9921 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9926 - val_dense_6_acc: 0.9910 - val_dense_7_acc: 0.9896 - val_dense_8_acc: 0.9830\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.24885 to 0.24085, saving model to seqence_wights.h5\n",
            "Epoch 100/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.7557 - dense_2_loss: 0.1044 - dense_3_loss: 0.1028 - dense_4_loss: 0.1079 - dense_5_loss: 0.1180 - dense_6_loss: 0.1177 - dense_7_loss: 0.1164 - dense_8_loss: 0.0886 - dense_2_acc: 0.9653 - dense_3_acc: 0.9659 - dense_4_acc: 0.9642 - dense_5_acc: 0.9608 - dense_6_acc: 0.9610 - dense_7_acc: 0.9617 - dense_8_acc: 0.9713 - val_loss: 0.2377 - val_dense_2_loss: 0.0213 - val_dense_3_loss: 0.0249 - val_dense_4_loss: 0.0239 - val_dense_5_loss: 0.0290 - val_dense_6_loss: 0.0355 - val_dense_7_loss: 0.0440 - val_dense_8_loss: 0.0592 - val_dense_2_acc: 0.9942 - val_dense_3_acc: 0.9929 - val_dense_4_acc: 0.9937 - val_dense_5_acc: 0.9921 - val_dense_6_acc: 0.9912 - val_dense_7_acc: 0.9899 - val_dense_8_acc: 0.9844\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.24085 to 0.23766, saving model to seqence_wights.h5\n",
            "Epoch 101/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.7477 - dense_2_loss: 0.1052 - dense_3_loss: 0.1044 - dense_4_loss: 0.1081 - dense_5_loss: 0.1162 - dense_6_loss: 0.1131 - dense_7_loss: 0.1127 - dense_8_loss: 0.0879 - dense_2_acc: 0.9652 - dense_3_acc: 0.9656 - dense_4_acc: 0.9642 - dense_5_acc: 0.9617 - dense_6_acc: 0.9624 - dense_7_acc: 0.9630 - dense_8_acc: 0.9714 - val_loss: 0.2373 - val_dense_2_loss: 0.0206 - val_dense_3_loss: 0.0243 - val_dense_4_loss: 0.0243 - val_dense_5_loss: 0.0274 - val_dense_6_loss: 0.0356 - val_dense_7_loss: 0.0445 - val_dense_8_loss: 0.0607 - val_dense_2_acc: 0.9945 - val_dense_3_acc: 0.9931 - val_dense_4_acc: 0.9939 - val_dense_5_acc: 0.9924 - val_dense_6_acc: 0.9903 - val_dense_7_acc: 0.9895 - val_dense_8_acc: 0.9830\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.23766 to 0.23735, saving model to seqence_wights.h5\n",
            "Epoch 102/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.7394 - dense_2_loss: 0.1048 - dense_3_loss: 0.1040 - dense_4_loss: 0.1038 - dense_5_loss: 0.1129 - dense_6_loss: 0.1125 - dense_7_loss: 0.1137 - dense_8_loss: 0.0877 - dense_2_acc: 0.9648 - dense_3_acc: 0.9648 - dense_4_acc: 0.9651 - dense_5_acc: 0.9627 - dense_6_acc: 0.9623 - dense_7_acc: 0.9623 - dense_8_acc: 0.9718 - val_loss: 0.2347 - val_dense_2_loss: 0.0208 - val_dense_3_loss: 0.0238 - val_dense_4_loss: 0.0249 - val_dense_5_loss: 0.0276 - val_dense_6_loss: 0.0355 - val_dense_7_loss: 0.0424 - val_dense_8_loss: 0.0597 - val_dense_2_acc: 0.9947 - val_dense_3_acc: 0.9937 - val_dense_4_acc: 0.9937 - val_dense_5_acc: 0.9928 - val_dense_6_acc: 0.9910 - val_dense_7_acc: 0.9906 - val_dense_8_acc: 0.9836\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.23735 to 0.23469, saving model to seqence_wights.h5\n",
            "Epoch 103/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.7383 - dense_2_loss: 0.1004 - dense_3_loss: 0.1020 - dense_4_loss: 0.1091 - dense_5_loss: 0.1151 - dense_6_loss: 0.1156 - dense_7_loss: 0.1085 - dense_8_loss: 0.0875 - dense_2_acc: 0.9669 - dense_3_acc: 0.9662 - dense_4_acc: 0.9639 - dense_5_acc: 0.9623 - dense_6_acc: 0.9610 - dense_7_acc: 0.9639 - dense_8_acc: 0.9711 - val_loss: 0.2352 - val_dense_2_loss: 0.0212 - val_dense_3_loss: 0.0235 - val_dense_4_loss: 0.0241 - val_dense_5_loss: 0.0284 - val_dense_6_loss: 0.0347 - val_dense_7_loss: 0.0433 - val_dense_8_loss: 0.0601 - val_dense_2_acc: 0.9940 - val_dense_3_acc: 0.9939 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9922 - val_dense_6_acc: 0.9916 - val_dense_7_acc: 0.9891 - val_dense_8_acc: 0.9830\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.23469\n",
            "Epoch 104/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.7175 - dense_2_loss: 0.0979 - dense_3_loss: 0.1013 - dense_4_loss: 0.1031 - dense_5_loss: 0.1112 - dense_6_loss: 0.1137 - dense_7_loss: 0.1048 - dense_8_loss: 0.0856 - dense_2_acc: 0.9670 - dense_3_acc: 0.9666 - dense_4_acc: 0.9653 - dense_5_acc: 0.9621 - dense_6_acc: 0.9622 - dense_7_acc: 0.9659 - dense_8_acc: 0.9721 - val_loss: 0.2207 - val_dense_2_loss: 0.0197 - val_dense_3_loss: 0.0219 - val_dense_4_loss: 0.0223 - val_dense_5_loss: 0.0258 - val_dense_6_loss: 0.0332 - val_dense_7_loss: 0.0401 - val_dense_8_loss: 0.0576 - val_dense_2_acc: 0.9944 - val_dense_3_acc: 0.9941 - val_dense_4_acc: 0.9933 - val_dense_5_acc: 0.9934 - val_dense_6_acc: 0.9918 - val_dense_7_acc: 0.9903 - val_dense_8_acc: 0.9832\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.23469 to 0.22067, saving model to seqence_wights.h5\n",
            "Epoch 105/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.7158 - dense_2_loss: 0.0991 - dense_3_loss: 0.1015 - dense_4_loss: 0.1024 - dense_5_loss: 0.1090 - dense_6_loss: 0.1115 - dense_7_loss: 0.1065 - dense_8_loss: 0.0859 - dense_2_acc: 0.9671 - dense_3_acc: 0.9660 - dense_4_acc: 0.9659 - dense_5_acc: 0.9644 - dense_6_acc: 0.9623 - dense_7_acc: 0.9644 - dense_8_acc: 0.9723 - val_loss: 0.2223 - val_dense_2_loss: 0.0206 - val_dense_3_loss: 0.0220 - val_dense_4_loss: 0.0219 - val_dense_5_loss: 0.0265 - val_dense_6_loss: 0.0333 - val_dense_7_loss: 0.0411 - val_dense_8_loss: 0.0569 - val_dense_2_acc: 0.9945 - val_dense_3_acc: 0.9942 - val_dense_4_acc: 0.9945 - val_dense_5_acc: 0.9933 - val_dense_6_acc: 0.9922 - val_dense_7_acc: 0.9900 - val_dense_8_acc: 0.9843\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.22067\n",
            "Epoch 106/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.7043 - dense_2_loss: 0.0973 - dense_3_loss: 0.0985 - dense_4_loss: 0.0971 - dense_5_loss: 0.1111 - dense_6_loss: 0.1108 - dense_7_loss: 0.1057 - dense_8_loss: 0.0839 - dense_2_acc: 0.9679 - dense_3_acc: 0.9671 - dense_4_acc: 0.9673 - dense_5_acc: 0.9627 - dense_6_acc: 0.9629 - dense_7_acc: 0.9649 - dense_8_acc: 0.9725 - val_loss: 0.2262 - val_dense_2_loss: 0.0210 - val_dense_3_loss: 0.0226 - val_dense_4_loss: 0.0228 - val_dense_5_loss: 0.0273 - val_dense_6_loss: 0.0344 - val_dense_7_loss: 0.0415 - val_dense_8_loss: 0.0566 - val_dense_2_acc: 0.9937 - val_dense_3_acc: 0.9940 - val_dense_4_acc: 0.9944 - val_dense_5_acc: 0.9929 - val_dense_6_acc: 0.9915 - val_dense_7_acc: 0.9902 - val_dense_8_acc: 0.9846\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.22067\n",
            "Epoch 107/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.7006 - dense_2_loss: 0.0952 - dense_3_loss: 0.0984 - dense_4_loss: 0.0998 - dense_5_loss: 0.1100 - dense_6_loss: 0.1099 - dense_7_loss: 0.1066 - dense_8_loss: 0.0807 - dense_2_acc: 0.9687 - dense_3_acc: 0.9674 - dense_4_acc: 0.9668 - dense_5_acc: 0.9639 - dense_6_acc: 0.9632 - dense_7_acc: 0.9647 - dense_8_acc: 0.9731 - val_loss: 0.2236 - val_dense_2_loss: 0.0207 - val_dense_3_loss: 0.0231 - val_dense_4_loss: 0.0234 - val_dense_5_loss: 0.0265 - val_dense_6_loss: 0.0321 - val_dense_7_loss: 0.0406 - val_dense_8_loss: 0.0573 - val_dense_2_acc: 0.9946 - val_dense_3_acc: 0.9939 - val_dense_4_acc: 0.9939 - val_dense_5_acc: 0.9933 - val_dense_6_acc: 0.9923 - val_dense_7_acc: 0.9907 - val_dense_8_acc: 0.9843\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.22067\n",
            "Epoch 108/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.6927 - dense_2_loss: 0.0954 - dense_3_loss: 0.0979 - dense_4_loss: 0.1030 - dense_5_loss: 0.1056 - dense_6_loss: 0.1067 - dense_7_loss: 0.1027 - dense_8_loss: 0.0813 - dense_2_acc: 0.9685 - dense_3_acc: 0.9667 - dense_4_acc: 0.9654 - dense_5_acc: 0.9641 - dense_6_acc: 0.9648 - dense_7_acc: 0.9658 - dense_8_acc: 0.9733 - val_loss: 0.2120 - val_dense_2_loss: 0.0194 - val_dense_3_loss: 0.0212 - val_dense_4_loss: 0.0213 - val_dense_5_loss: 0.0259 - val_dense_6_loss: 0.0309 - val_dense_7_loss: 0.0384 - val_dense_8_loss: 0.0550 - val_dense_2_acc: 0.9942 - val_dense_3_acc: 0.9944 - val_dense_4_acc: 0.9944 - val_dense_5_acc: 0.9926 - val_dense_6_acc: 0.9917 - val_dense_7_acc: 0.9909 - val_dense_8_acc: 0.9839\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.22067 to 0.21198, saving model to seqence_wights.h5\n",
            "Epoch 109/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.6761 - dense_2_loss: 0.0936 - dense_3_loss: 0.0949 - dense_4_loss: 0.0942 - dense_5_loss: 0.1058 - dense_6_loss: 0.1062 - dense_7_loss: 0.1023 - dense_8_loss: 0.0791 - dense_2_acc: 0.9688 - dense_3_acc: 0.9684 - dense_4_acc: 0.9687 - dense_5_acc: 0.9649 - dense_6_acc: 0.9644 - dense_7_acc: 0.9658 - dense_8_acc: 0.9742 - val_loss: 0.2036 - val_dense_2_loss: 0.0193 - val_dense_3_loss: 0.0210 - val_dense_4_loss: 0.0207 - val_dense_5_loss: 0.0239 - val_dense_6_loss: 0.0302 - val_dense_7_loss: 0.0363 - val_dense_8_loss: 0.0524 - val_dense_2_acc: 0.9946 - val_dense_3_acc: 0.9942 - val_dense_4_acc: 0.9948 - val_dense_5_acc: 0.9939 - val_dense_6_acc: 0.9926 - val_dense_7_acc: 0.9915 - val_dense_8_acc: 0.9850\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.21198 to 0.20363, saving model to seqence_wights.h5\n",
            "Epoch 110/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.6758 - dense_2_loss: 0.0938 - dense_3_loss: 0.0941 - dense_4_loss: 0.0977 - dense_5_loss: 0.1048 - dense_6_loss: 0.1052 - dense_7_loss: 0.0998 - dense_8_loss: 0.0803 - dense_2_acc: 0.9690 - dense_3_acc: 0.9687 - dense_4_acc: 0.9677 - dense_5_acc: 0.9649 - dense_6_acc: 0.9652 - dense_7_acc: 0.9666 - dense_8_acc: 0.9730 - val_loss: 0.2104 - val_dense_2_loss: 0.0207 - val_dense_3_loss: 0.0210 - val_dense_4_loss: 0.0207 - val_dense_5_loss: 0.0251 - val_dense_6_loss: 0.0308 - val_dense_7_loss: 0.0375 - val_dense_8_loss: 0.0547 - val_dense_2_acc: 0.9937 - val_dense_3_acc: 0.9943 - val_dense_4_acc: 0.9947 - val_dense_5_acc: 0.9936 - val_dense_6_acc: 0.9915 - val_dense_7_acc: 0.9909 - val_dense_8_acc: 0.9841\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.20363\n",
            "Epoch 111/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.6595 - dense_2_loss: 0.0905 - dense_3_loss: 0.0940 - dense_4_loss: 0.0951 - dense_5_loss: 0.1019 - dense_6_loss: 0.1010 - dense_7_loss: 0.0988 - dense_8_loss: 0.0781 - dense_2_acc: 0.9700 - dense_3_acc: 0.9689 - dense_4_acc: 0.9690 - dense_5_acc: 0.9653 - dense_6_acc: 0.9663 - dense_7_acc: 0.9673 - dense_8_acc: 0.9737 - val_loss: 0.2018 - val_dense_2_loss: 0.0202 - val_dense_3_loss: 0.0199 - val_dense_4_loss: 0.0201 - val_dense_5_loss: 0.0231 - val_dense_6_loss: 0.0292 - val_dense_7_loss: 0.0357 - val_dense_8_loss: 0.0536 - val_dense_2_acc: 0.9940 - val_dense_3_acc: 0.9947 - val_dense_4_acc: 0.9947 - val_dense_5_acc: 0.9934 - val_dense_6_acc: 0.9926 - val_dense_7_acc: 0.9905 - val_dense_8_acc: 0.9842\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.20363 to 0.20182, saving model to seqence_wights.h5\n",
            "Epoch 112/120\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.6595 - dense_2_loss: 0.0916 - dense_3_loss: 0.0922 - dense_4_loss: 0.0952 - dense_5_loss: 0.0974 - dense_6_loss: 0.1013 - dense_7_loss: 0.1044 - dense_8_loss: 0.0774 - dense_2_acc: 0.9700 - dense_3_acc: 0.9690 - dense_4_acc: 0.9679 - dense_5_acc: 0.9670 - dense_6_acc: 0.9654 - dense_7_acc: 0.9645 - dense_8_acc: 0.9741 - val_loss: 0.2022 - val_dense_2_loss: 0.0180 - val_dense_3_loss: 0.0190 - val_dense_4_loss: 0.0200 - val_dense_5_loss: 0.0228 - val_dense_6_loss: 0.0291 - val_dense_7_loss: 0.0394 - val_dense_8_loss: 0.0539 - val_dense_2_acc: 0.9947 - val_dense_3_acc: 0.9944 - val_dense_4_acc: 0.9950 - val_dense_5_acc: 0.9938 - val_dense_6_acc: 0.9922 - val_dense_7_acc: 0.9895 - val_dense_8_acc: 0.9846\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.20182\n",
            "Epoch 113/120\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.6572 - dense_2_loss: 0.0910 - dense_3_loss: 0.0920 - dense_4_loss: 0.0982 - dense_5_loss: 0.1003 - dense_6_loss: 0.1018 - dense_7_loss: 0.0978 - dense_8_loss: 0.0761 - dense_2_acc: 0.9701 - dense_3_acc: 0.9699 - dense_4_acc: 0.9673 - dense_5_acc: 0.9665 - dense_6_acc: 0.9662 - dense_7_acc: 0.9681 - dense_8_acc: 0.9749 - val_loss: 0.1991 - val_dense_2_loss: 0.0186 - val_dense_3_loss: 0.0201 - val_dense_4_loss: 0.0194 - val_dense_5_loss: 0.0232 - val_dense_6_loss: 0.0298 - val_dense_7_loss: 0.0361 - val_dense_8_loss: 0.0519 - val_dense_2_acc: 0.9947 - val_dense_3_acc: 0.9949 - val_dense_4_acc: 0.9950 - val_dense_5_acc: 0.9935 - val_dense_6_acc: 0.9925 - val_dense_7_acc: 0.9907 - val_dense_8_acc: 0.9850\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.20182 to 0.19910, saving model to seqence_wights.h5\n",
            "Epoch 114/120\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.6468 - dense_2_loss: 0.0884 - dense_3_loss: 0.0906 - dense_4_loss: 0.0942 - dense_5_loss: 0.1000 - dense_6_loss: 0.0994 - dense_7_loss: 0.0973 - dense_8_loss: 0.0769 - dense_2_acc: 0.9701 - dense_3_acc: 0.9694 - dense_4_acc: 0.9683 - dense_5_acc: 0.9665 - dense_6_acc: 0.9666 - dense_7_acc: 0.9682 - dense_8_acc: 0.9743 - val_loss: 0.1953 - val_dense_2_loss: 0.0173 - val_dense_3_loss: 0.0206 - val_dense_4_loss: 0.0190 - val_dense_5_loss: 0.0231 - val_dense_6_loss: 0.0285 - val_dense_7_loss: 0.0346 - val_dense_8_loss: 0.0522 - val_dense_2_acc: 0.9945 - val_dense_3_acc: 0.9936 - val_dense_4_acc: 0.9948 - val_dense_5_acc: 0.9939 - val_dense_6_acc: 0.9923 - val_dense_7_acc: 0.9914 - val_dense_8_acc: 0.9846\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.19910 to 0.19532, saving model to seqence_wights.h5\n",
            "Epoch 115/120\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.6372 - dense_2_loss: 0.0881 - dense_3_loss: 0.0891 - dense_4_loss: 0.0907 - dense_5_loss: 0.0986 - dense_6_loss: 0.0976 - dense_7_loss: 0.0967 - dense_8_loss: 0.0763 - dense_2_acc: 0.9700 - dense_3_acc: 0.9705 - dense_4_acc: 0.9701 - dense_5_acc: 0.9670 - dense_6_acc: 0.9673 - dense_7_acc: 0.9676 - dense_8_acc: 0.9745 - val_loss: 0.1960 - val_dense_2_loss: 0.0183 - val_dense_3_loss: 0.0189 - val_dense_4_loss: 0.0187 - val_dense_5_loss: 0.0240 - val_dense_6_loss: 0.0289 - val_dense_7_loss: 0.0361 - val_dense_8_loss: 0.0511 - val_dense_2_acc: 0.9950 - val_dense_3_acc: 0.9950 - val_dense_4_acc: 0.9955 - val_dense_5_acc: 0.9932 - val_dense_6_acc: 0.9921 - val_dense_7_acc: 0.9905 - val_dense_8_acc: 0.9855\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.19532\n",
            "Epoch 116/120\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.6320 - dense_2_loss: 0.0898 - dense_3_loss: 0.0870 - dense_4_loss: 0.0891 - dense_5_loss: 0.0996 - dense_6_loss: 0.0971 - dense_7_loss: 0.0952 - dense_8_loss: 0.0743 - dense_2_acc: 0.9703 - dense_3_acc: 0.9709 - dense_4_acc: 0.9696 - dense_5_acc: 0.9667 - dense_6_acc: 0.9669 - dense_7_acc: 0.9677 - dense_8_acc: 0.9762 - val_loss: 0.1906 - val_dense_2_loss: 0.0173 - val_dense_3_loss: 0.0192 - val_dense_4_loss: 0.0185 - val_dense_5_loss: 0.0224 - val_dense_6_loss: 0.0283 - val_dense_7_loss: 0.0345 - val_dense_8_loss: 0.0505 - val_dense_2_acc: 0.9951 - val_dense_3_acc: 0.9947 - val_dense_4_acc: 0.9952 - val_dense_5_acc: 0.9933 - val_dense_6_acc: 0.9927 - val_dense_7_acc: 0.9913 - val_dense_8_acc: 0.9856\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.19532 to 0.19061, saving model to seqence_wights.h5\n",
            "Epoch 117/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.6367 - dense_2_loss: 0.0875 - dense_3_loss: 0.0883 - dense_4_loss: 0.0920 - dense_5_loss: 0.0997 - dense_6_loss: 0.0985 - dense_7_loss: 0.0955 - dense_8_loss: 0.0753 - dense_2_acc: 0.9715 - dense_3_acc: 0.9708 - dense_4_acc: 0.9695 - dense_5_acc: 0.9663 - dense_6_acc: 0.9676 - dense_7_acc: 0.9679 - dense_8_acc: 0.9749 - val_loss: 0.1901 - val_dense_2_loss: 0.0172 - val_dense_3_loss: 0.0200 - val_dense_4_loss: 0.0187 - val_dense_5_loss: 0.0230 - val_dense_6_loss: 0.0293 - val_dense_7_loss: 0.0323 - val_dense_8_loss: 0.0496 - val_dense_2_acc: 0.9954 - val_dense_3_acc: 0.9945 - val_dense_4_acc: 0.9952 - val_dense_5_acc: 0.9939 - val_dense_6_acc: 0.9925 - val_dense_7_acc: 0.9918 - val_dense_8_acc: 0.9863\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.19061 to 0.19006, saving model to seqence_wights.h5\n",
            "Epoch 118/120\n",
            "60000/60000 [==============================] - 11s 192us/step - loss: 0.6199 - dense_2_loss: 0.0870 - dense_3_loss: 0.0870 - dense_4_loss: 0.0882 - dense_5_loss: 0.0926 - dense_6_loss: 0.0984 - dense_7_loss: 0.0940 - dense_8_loss: 0.0727 - dense_2_acc: 0.9716 - dense_3_acc: 0.9714 - dense_4_acc: 0.9706 - dense_5_acc: 0.9688 - dense_6_acc: 0.9675 - dense_7_acc: 0.9685 - dense_8_acc: 0.9764 - val_loss: 0.1832 - val_dense_2_loss: 0.0177 - val_dense_3_loss: 0.0182 - val_dense_4_loss: 0.0177 - val_dense_5_loss: 0.0225 - val_dense_6_loss: 0.0277 - val_dense_7_loss: 0.0318 - val_dense_8_loss: 0.0476 - val_dense_2_acc: 0.9949 - val_dense_3_acc: 0.9947 - val_dense_4_acc: 0.9956 - val_dense_5_acc: 0.9938 - val_dense_6_acc: 0.9933 - val_dense_7_acc: 0.9926 - val_dense_8_acc: 0.9867\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.19006 to 0.18318, saving model to seqence_wights.h5\n",
            "Epoch 119/120\n",
            "60000/60000 [==============================] - 11s 188us/step - loss: 0.6202 - dense_2_loss: 0.0869 - dense_3_loss: 0.0878 - dense_4_loss: 0.0905 - dense_5_loss: 0.0946 - dense_6_loss: 0.0965 - dense_7_loss: 0.0918 - dense_8_loss: 0.0723 - dense_2_acc: 0.9706 - dense_3_acc: 0.9705 - dense_4_acc: 0.9703 - dense_5_acc: 0.9682 - dense_6_acc: 0.9677 - dense_7_acc: 0.9699 - dense_8_acc: 0.9763 - val_loss: 0.1781 - val_dense_2_loss: 0.0163 - val_dense_3_loss: 0.0178 - val_dense_4_loss: 0.0175 - val_dense_5_loss: 0.0213 - val_dense_6_loss: 0.0267 - val_dense_7_loss: 0.0318 - val_dense_8_loss: 0.0468 - val_dense_2_acc: 0.9954 - val_dense_3_acc: 0.9952 - val_dense_4_acc: 0.9952 - val_dense_5_acc: 0.9935 - val_dense_6_acc: 0.9933 - val_dense_7_acc: 0.9917 - val_dense_8_acc: 0.9869\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.18318 to 0.17815, saving model to seqence_wights.h5\n",
            "Epoch 120/120\n",
            "60000/60000 [==============================] - 11s 187us/step - loss: 0.6160 - dense_2_loss: 0.0868 - dense_3_loss: 0.0858 - dense_4_loss: 0.0887 - dense_5_loss: 0.0932 - dense_6_loss: 0.0963 - dense_7_loss: 0.0906 - dense_8_loss: 0.0746 - dense_2_acc: 0.9713 - dense_3_acc: 0.9702 - dense_4_acc: 0.9705 - dense_5_acc: 0.9694 - dense_6_acc: 0.9677 - dense_7_acc: 0.9699 - dense_8_acc: 0.9757 - val_loss: 0.1816 - val_dense_2_loss: 0.0165 - val_dense_3_loss: 0.0196 - val_dense_4_loss: 0.0175 - val_dense_5_loss: 0.0209 - val_dense_6_loss: 0.0265 - val_dense_7_loss: 0.0338 - val_dense_8_loss: 0.0469 - val_dense_2_acc: 0.9957 - val_dense_3_acc: 0.9942 - val_dense_4_acc: 0.9957 - val_dense_5_acc: 0.9940 - val_dense_6_acc: 0.9932 - val_dense_7_acc: 0.9913 - val_dense_8_acc: 0.9862\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.17815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbcdcd7a4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ECZU4iF0xpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjycJaC303UH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "815250df-5077-4c45-cebe-881db10f3c97"
      },
      "source": [
        "np.shape(predictions)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 20000, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQZmXpVU07zx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff8692af-1f7e-4c54-d52d-2362a928debd"
      },
      "source": [
        "np.shape(test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 20000, 37)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0E6U3eu1BBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4362a6d7-9e7c-45c6-db5d-a9ebcf381b03"
      },
      "source": [
        "def calculate_acc(predictions,real_labels):\n",
        "    \n",
        "    individual_counter = 0\n",
        "    global_sequence_counter = 0\n",
        "    for i in range(0,len(predictions[0])):\n",
        "        #Reset sequence counter at the start of each image\n",
        "        sequence_counter = 0 \n",
        "        \n",
        "        for j in range(0,7):\n",
        "            if np.argmax(predictions[j][i]) == np.argmax(real_labels[j][i]):\n",
        "                individual_counter += 1\n",
        "                sequence_counter +=1\n",
        "        \n",
        "        if sequence_counter == 7:\n",
        "            global_sequence_counter += 1\n",
        "         \n",
        "    ind_accuracy = individual_counter/140000.0\n",
        "    global_accuracy = global_sequence_counter/20000.0\n",
        "    \n",
        "    return ind_accuracy,global_accuracy\n",
        "ind_acc,glob_acc = calculate_acc(predictions,test_labels)\n",
        "\n",
        "print(\"The individual accuracy is {} %\".format(ind_acc*100))\n",
        "print(\"The sequence prediction accuracy is {} %\".format(glob_acc*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The individual accuracy is 99.28857142857143 %\n",
            "The sequence prediction accuracy is 95.56 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3MrZN8P1yVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d14f8f11-62a6-4ccb-fc0a-18d03528b3a2"
      },
      "source": [
        "#Printing some examples of real and predicted labels\n",
        "for i in random.sample(range(0,20000),7):\n",
        "    \n",
        "    actual_labels = []\n",
        "    predicted_labels = []\n",
        "    \n",
        "    for j in range(0,7):\n",
        "        actual_labels.append(np.argmax(test_labels[j][i]))\n",
        "        predicted_labels.append(np.argmax(predictions[j][i]))\n",
        "        \n",
        "    print(\"Actual labels: {}\".format(actual_labels))\n",
        "    print(\"Predicted labels: {}\\n\".format(predicted_labels))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual labels: [8, 5, 20, 10, 10, 10, 10]\n",
            "Predicted labels: [8, 5, 20, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [11, 10, 10, 10, 10, 10, 10]\n",
            "Predicted labels: [11, 10, 10, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [13, 10, 10, 10, 10, 10, 10]\n",
            "Predicted labels: [13, 10, 10, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [1, 10, 16, 10, 10, 10, 10]\n",
            "Predicted labels: [1, 10, 16, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [31, 20, 10, 10, 10, 10, 10]\n",
            "Predicted labels: [31, 20, 10, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [13, 33, 6, 10, 10, 10, 10]\n",
            "Predicted labels: [13, 33, 6, 10, 10, 10, 10]\n",
            "\n",
            "Actual labels: [10, 26, 27, 13, 8, 26, 10]\n",
            "Predicted labels: [10, 26, 27, 13, 8, 26, 10]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB7tWK7X158T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "49f85898-257b-4e2a-dc23-6a2da8a4c73b"
      },
      "source": [
        "im = cv2.imread('CAB9614_2.jpg', 0)\n",
        "\n",
        "new_image = cv2.resize(im, (196,28), interpolation = cv2.INTER_AREA)\n",
        "img2 = cv2.normalize(new_image,None,90,120,cv2.NORM_MINMAX)\n",
        "blurImg = cv2.blur(new_image,(2,2)) \n",
        "\n",
        "print(blurImg.size)\n",
        "img_data = blurImg.reshape(1,196,28,1)\n",
        "#Converting everything to floats\n",
        "img_data = img_data.astype('float32')   \n",
        "#Normalizing values between 0 and 1\n",
        "img_data /= 255\n",
        "\n",
        "\n",
        "predictions = model.predict(img_data)\n",
        "print(np.shape(predictions))\n",
        "\n",
        "predicted_labels = []\n",
        " \n",
        "for j in range(0,7):\n",
        "  predicted_labels.append(np.argmax(predictions[j]))\n",
        "        \n",
        "print(\"Predicted labels: {}\\n\".format(predicted_labels))\n",
        "\n",
        "plt.imshow(blurImg)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5488\n",
            "(7, 1, 37)\n",
            "Predicted labels: [25, 9, 2, 10, 1, 10, 10]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABSCAYAAABNCo+2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncP8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}